{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b788a66",
   "metadata": {},
   "source": [
    "# Wikipedia LLM Training Notebook\n",
    "\n",
    "This notebook contains the complete pipeline for training a Wikipedia-based language model using Phi-2 with LoRA fine-tuning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Package Installation](#Package-Installation)\n",
    "2. [GPU Check](#GPU-Check)\n",
    "3. [Data Download](#Data-Download)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Model Setup](#Model-Setup)\n",
    "6. [LoRA Configuration](#LoRA-Configuration)\n",
    "7. [Training Progress Check](#Training-Progress-Check)\n",
    "8. [Model Training](#Model-Training)\n",
    "9. [VRAM Cleanup](#VRAM-Cleanup)\n",
    "10. [Model Testing](#Model-Testing)\n",
    "11. [Chat Interface](#Chat-Interface)\n",
    "12. [Training Restart](#Training-Restart)\n",
    "13. [Hyperparameter Tuning](#Hyperparameter-Tuning)\n",
    "14. [Model Evaluation](#Model-Evaluation)\n",
    "15. [Data Analysis](#Data-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263af3e",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "\n",
    "Installs all required Python packages for the training pipeline, including PyTorch, Transformers, PEFT, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for RTX 5060 Ti GPU (Uncomment to use in a Jupyter notebook)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "!pip install transformers datasets peft accelerate bitsandbytes trl tqdm protobuf scipy sentencepiece psutil matplotlib mlflow rouge-score nltk wordcloud seaborn pandas tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600969",
   "metadata": {},
   "source": [
    "## GPU Check\n",
    "\n",
    "Verifies CUDA availability and GPU information to ensure the system is ready for GPU-accelerated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44de41",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "Downloads a subset of the Wikipedia dataset (100,000 articles) from Hugging Face and saves it locally for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Downloading Wikipedia dataset (this will take a while)...\")\n",
    "dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.en\",  # November 2023 snapshot\n",
    "    split=\"train\",\n",
    "    streaming=False  # Set to True if to stream instead of download all\n",
    ")\n",
    "\n",
    "# Take subset for testing (adjust as needed)\n",
    "print(\"Creating subset...\")\n",
    "subset = dataset.select(range(min(100000, len(dataset))))\n",
    "\n",
    "print(\"Saving dataset locally...\")\n",
    "subset.save_to_disk(\"data/wikipedia_100k\")\n",
    "print(f\"Dataset saved! Total articles: {len(subset)}\")\n",
    "print(f\"Sample article title: {subset[0]['title']}\")\n",
    "print(f\"Sample text: {subset[0]['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f002d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Processes the raw Wikipedia data with text cleaning, quality filtering, and multiple instruction formats for better training data quality and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9516b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load saved dataset\n",
    "dataset = load_from_disk(\"data/wikipedia_100k\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove wiki markup\n",
    "    text = re.sub(r'\\[\\[([^\\]|]*\\|)?([^\\]]*)\\]\\]', r'\\2', text)  # Remove [[links]]\n",
    "    text = re.sub(r\"''+\", '', text)  # Remove italic/bold markup\n",
    "    return text\n",
    "\n",
    "def format_for_training(example):\n",
    "    \"\"\"Convert Wikipedia articles to multiple instruction formats\"\"\"\n",
    "    text = clean_text(example['text'])\n",
    "    title = example['title']\n",
    "    \n",
    "    # Skip very short articles\n",
    "    if len(text) < 200:\n",
    "        return {\"text\": \"\"}  # Will be filtered out\n",
    "    \n",
    "    # Create multiple instruction formats for variety\n",
    "    formats = [\n",
    "        f\"### Instruction:\\nProvide information about {title}.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nExplain what {title} is.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nTell me about {title}.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nGive me details on {title}.\\n\\n### Response:\\n{text[:1000]}\"\n",
    "    ]\n",
    "    \n",
    "    # Randomly select one format\n",
    "    return {\"text\": random.choice(formats)}\n",
    "\n",
    "# Format dataset\n",
    "print(\"Formatting and cleaning dataset...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    format_for_training,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Filter out empty entries\n",
    "formatted_dataset = formatted_dataset.filter(lambda x: len(x['text']) > 50)\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Save formatted data\n",
    "split_dataset.save_to_disk(\"data/formatted_wikipedia\")\n",
    "print(\"Data preparation complete!\")\n",
    "print(f\"Training samples: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(split_dataset['test'])}\")\n",
    "print(f\"Filtered out {len(dataset) - len(formatted_dataset)} short/invalid articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708807d",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Loads the Phi-2 model with 4-bit quantization, performs system resource checks, memory estimation, and provides model merging capabilities for optimized inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "# Choose a base model (recommendations for your hardware)\n",
    "MODEL_OPTIONS = {\n",
    "    \"small\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B - easiest to train\n",
    "    \"medium\": \"microsoft/phi-2\",  # 2.7B - good balance\n",
    "    \"large\": \"mistralai/Mistral-7B-v0.1\"  # 7B - needs quantization\n",
    "}\n",
    "\n",
    "model_name = MODEL_OPTIONS[\"medium\"]  # Start with Phi-2\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "def estimate_memory_usage(model_name, quantized=True):\n",
    "    \"\"\"Estimate memory usage for the model\"\"\"\n",
    "    # Rough estimates (in GB)\n",
    "    base_sizes = {\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\": 2.2,\n",
    "        \"microsoft/phi-2\": 5.0,\n",
    "        \"mistralai/Mistral-7B-v0.1\": 14.0\n",
    "    }\n",
    "    \n",
    "    base_size = base_sizes.get(model_name, 5.0)\n",
    "    if quantized:\n",
    "        estimated_vram = base_size * 0.3  # 4-bit quantization reduces to ~30%\n",
    "    else:\n",
    "        estimated_vram = base_size\n",
    "    \n",
    "    return estimated_vram\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"Check available system resources\"\"\"\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "    cpu_memory = psutil.virtual_memory().total / 1024**3\n",
    "    \n",
    "    print(f\"Available GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"Available CPU Memory: {cpu_memory:.1f} GB\")\n",
    "    return gpu_memory, cpu_memory\n",
    "\n",
    "# Check resources before loading\n",
    "print(\"Checking system resources...\")\n",
    "gpu_mem, cpu_mem = check_system_resources()\n",
    "\n",
    "estimated_vram = estimate_memory_usage(model_name, quantized=True)\n",
    "print(f\"Estimated VRAM usage: {estimated_vram:.1f} GB\")\n",
    "\n",
    "if gpu_mem > 0 and estimated_vram > gpu_mem * 0.8:\n",
    "    print(\"WARNING: Estimated VRAM usage is high. Consider using smaller model or more aggressive quantization.\")\n",
    "elif gpu_mem == 0:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow on CPU.\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05a4d1",
   "metadata": {},
   "source": [
    "## Optional: Model Merging\n",
    "\n",
    "This section provides functionality to merge LoRA weights back into the base model for faster inference after training.\n",
    "\n",
    "### Usage Instructions:\n",
    "\n",
    "1. Train your LoRA model using the training cells below.\n",
    "2. After training completes, run the code cell below to merge the LoRA weights.\n",
    "3. Provide the base model (loaded above), the path to your trained LoRA weights, and a save path for the merged model.\n",
    "4. Example: `merged_model = merge_lora_weights(model, './wikipedia_model/final', './wikipedia_model/merged')`\n",
    "5. The merged model will be saved and can be used for faster inference without LoRA adapters.\n",
    "\n",
    "**Note:** This step is optional. If you prefer to keep the model in LoRA format for flexibility, you can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(base_model, lora_path, save_path):\n",
    "    \"\"\"Merge LoRA weights back into base model for faster inference\"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(\"Loading LoRA weights...\")\n",
    "    lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    print(\"Merging weights...\")\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    \n",
    "    print(f\"Saving merged model to {save_path}...\")\n",
    "    merged_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(\"Model merging complete!\")\n",
    "    return merged_model\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "merged_model = merge_lora_weights(model, \"./wikipedia_model/final\", \"./wikipedia_model/merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20c963",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Configures Low-Rank Adaptation with dynamic rank scaling, expanded target modules, and enhanced regularization for optimal fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Dynamic LoRA rank based on model size\n",
    "def get_dynamic_lora_rank(model):\n",
    "    \"\"\"Determine LoRA rank based on model parameter count\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if param_count < 2e9:  # < 2B params\n",
    "        rank = 8\n",
    "    elif param_count < 7e9:  # 2-7B params\n",
    "        rank = 16\n",
    "    else:  # > 7B params\n",
    "        rank = 32\n",
    "    \n",
    "    return rank\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Get dynamic rank\n",
    "lora_rank = get_dynamic_lora_rank(model)\n",
    "print(f\"Using LoRA rank: {lora_rank} (based on model size)\")\n",
    "\n",
    "# Enhanced LoRA configuration with more target modules\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,  # Dynamic rank\n",
    "    lora_alpha=lora_rank * 2,  # Typically 2x rank\n",
    "    target_modules=[\n",
    "        \"q_proj\",   # Query projection\n",
    "        \"k_proj\",   # Key projection  \n",
    "        \"v_proj\",   # Value projection\n",
    "        \"o_proj\",   # Output projection\n",
    "        \"gate_proj\",  # Gate projection (for some models)\n",
    "        \"up_proj\",    # Up projection (for some models)\n",
    "        \"down_proj\",  # Down projection (for some models)\n",
    "    ],\n",
    "    lora_dropout=0.1,  # Slightly higher dropout for better regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# LoRA dropout scheduling (optional - requires custom training loop)\n",
    "# This would need to be implemented in the training loop to decrease dropout over time\n",
    "def get_scheduled_dropout(epoch, max_epochs, initial_dropout=0.1, final_dropout=0.01):\n",
    "    \"\"\"Linearly decrease dropout from initial to final over training\"\"\"\n",
    "    return initial_dropout - (initial_dropout - final_dropout) * (epoch / max_epochs)\n",
    "\n",
    "print(f\"\\nLoRA Configuration Summary:\")\n",
    "print(f\"- Rank: {lora_rank}\")\n",
    "print(f\"- Alpha: {lora_rank * 2}\")\n",
    "print(f\"- Target Modules: {len(lora_config.target_modules)} modules\")\n",
    "print(f\"- Dropout: {lora_config.lora_dropout}\")\n",
    "print(\"- Bias: none\")\n",
    "print(\"- Task: Causal LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d4456",
   "metadata": {},
   "source": [
    "## Training Progress Check\n",
    "\n",
    "Analyzes existing checkpoints with loss curve visualization, training speed metrics, and live model performance preview through sample generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e68ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"Check current training progress with enhanced metrics and visualization\"\"\"\n",
    "    \n",
    "    output_dir = \"./wikipedia_model\"\n",
    "    \n",
    "    # Find all checkpoints\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found. Training hasn't started yet.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    \n",
    "    print(f\"Total checkpoints found: {len(checkpoints)}\")\n",
    "    print(\"\\nCheckpoint history:\")\n",
    "    \n",
    "    # Collect data for plotting\n",
    "    steps = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    eval_losses = []\n",
    "    \n",
    "    for cp in checkpoints:\n",
    "        step = cp.split(\"-\")[-1]\n",
    "        \n",
    "        # Try to read trainer_state.json for more info\n",
    "        state_file = os.path.join(cp, \"trainer_state.json\")\n",
    "        if os.path.exists(state_file):\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "                epoch = state.get('epoch', 'N/A')\n",
    "                print(f\"  - Step {step} (Epoch {epoch:.2f})\")\n",
    "                \n",
    "                # Collect training metrics\n",
    "                log_history = state.get('log_history', [])\n",
    "                training_entries = [entry for entry in log_history if 'loss' in entry]\n",
    "                eval_entries = [entry for entry in log_history if 'eval_loss' in entry]\n",
    "                \n",
    "                if training_entries:\n",
    "                    last_train = training_entries[-1]\n",
    "                    steps.append(int(step))\n",
    "                    losses.append(last_train.get('loss', 0))\n",
    "                    learning_rates.append(last_train.get('learning_rate', 0))\n",
    "                \n",
    "                if eval_entries:\n",
    "                    last_eval = eval_entries[-1]\n",
    "                    eval_losses.append(last_eval.get('eval_loss', 0))\n",
    "        else:\n",
    "            print(f\"  - Step {step}\")\n",
    "    \n",
    "    # Latest checkpoint\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"\\nLatest checkpoint: {latest}\")\n",
    "    \n",
    "    # Read detailed info from latest\n",
    "    state_file = os.path.join(latest, \"trainer_state.json\")\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            print(f\"\\nDetailed Progress:\")\n",
    "            print(f\"  Current Step: {state.get('global_step', 'N/A')}\")\n",
    "            print(f\"  Current Epoch: {state.get('epoch', 'N/A'):.2f}\")\n",
    "            \n",
    "            # Get latest training metrics (find last entry with training loss)\n",
    "            log_history = state.get('log_history', [])\n",
    "            training_entries = [entry for entry in log_history if 'loss' in entry]\n",
    "            eval_entries = [entry for entry in log_history if 'eval_loss' in entry]\n",
    "            \n",
    "            if training_entries:\n",
    "                last_training = training_entries[-1]\n",
    "                training_loss = last_training.get('loss', 'N/A')\n",
    "                learning_rate = last_training.get('learning_rate', 'N/A')\n",
    "            else:\n",
    "                training_loss = 'N/A'\n",
    "                learning_rate = 'N/A'\n",
    "            \n",
    "            if eval_entries:\n",
    "                last_eval = eval_entries[-1]\n",
    "                validation_loss = last_eval.get('eval_loss', 'N/A')\n",
    "            else:\n",
    "                validation_loss = 'N/A'\n",
    "            \n",
    "            print(f\"  Training Loss: {training_loss}\")\n",
    "            print(f\"  Validation Loss: {validation_loss}\")\n",
    "            print(f\"  Learning Rate: {learning_rate}\")\n",
    "            \n",
    "            # Calculate progress percentage\n",
    "            max_steps = state.get('max_steps', None)\n",
    "            if max_steps:\n",
    "                current_step = state.get('global_step', 0)\n",
    "                percentage = (current_step / max_steps) * 100\n",
    "                print(f\"  Progress: {percentage:.1f}% ({current_step}/{max_steps} steps)\")\n",
    "                \n",
    "                # Estimate training speed\n",
    "                if len(steps) > 1:\n",
    "                    steps_per_hour = (steps[-1] - steps[0]) / ((len(steps) - 1) * 0.1)  # Rough estimate\n",
    "                    print(f\"  Estimated Speed: ~{steps_per_hour:.1f} steps/hour\")\n",
    "            else:\n",
    "                print(\"  Progress: Unable to calculate (max_steps not available)\")\n",
    "            \n",
    "            # Plot loss curves if we have data\n",
    "            if len(losses) > 1:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                \n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.plot(steps, losses, 'b-', label='Training Loss')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Training Loss Curve')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.plot(steps, learning_rates, 'r-', label='Learning Rate')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('Learning Rate Schedule')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                if eval_losses:\n",
    "                    eval_steps = steps[:len(eval_losses)]  # Assume eval at same steps\n",
    "                    plt.plot(eval_steps, eval_losses, 'g-', label='Validation Loss')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Validation Loss')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Show recent training history (last 3 training entries with validation loss)\n",
    "            if training_entries:\n",
    "                print(f\"\\nRecent Training History (last {min(3, len(training_entries))} entries):\")\n",
    "                for entry in training_entries[-3:]:\n",
    "                    step = entry.get('step', 'N/A')\n",
    "                    loss = entry.get('loss', 'N/A')\n",
    "                    lr = entry.get('learning_rate', 'N/A')\n",
    "                    epoch = entry.get('epoch', 'N/A')\n",
    "                    \n",
    "                    # Find corresponding validation loss for this step\n",
    "                    val_loss = 'N/A'\n",
    "                    for eval_entry in eval_entries:\n",
    "                        if eval_entry.get('step') == step:\n",
    "                            val_loss = eval_entry.get('eval_loss', 'N/A')\n",
    "                            break\n",
    "                    \n",
    "                    print(f\"    Step {step}: Train Loss={loss}, Val Loss={val_loss}, LR={lr}, Epoch={epoch:.2f}\")\n",
    "            else:\n",
    "                print(\"\\nNo training history available yet.\")\n",
    "            \n",
    "            # Generate sample output from latest checkpoint\n",
    "            print(f\"\\nSample Generation from Latest Checkpoint:\")\n",
    "            try:\n",
    "                # Load model for inference\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"microsoft/phi-2\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                lora_model = PeftModel.from_pretrained(base_model, latest)\n",
    "                test_tokenizer = AutoTokenizer.from_pretrained(latest)\n",
    "                \n",
    "                # Test prompt\n",
    "                prompt = \"### Instruction:\\nProvide information about Artificial Intelligence.\\n\\n### Response:\\n\"\n",
    "                inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = lora_model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=100,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=test_tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # Extract just the response part\n",
    "                answer = response.split(\"### Response:\\n\")[-1][:200] + \"...\"\n",
    "                print(f\"AI: {answer}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del base_model, lora_model\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate sample: {e}\")\n",
    "    \n",
    "    print(\"\\nTo resume training, simply run: python train.py\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f73d9f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Executes the main training loop with optimized settings including gradient checkpointing, mixed precision, and automatic checkpoint resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c487cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_vram():\n",
    "    \"\"\"Clear VRAM before training\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"VRAM cleared\")\n",
    "\n",
    "# Call it before loading model\n",
    "clear_vram()\n",
    "\n",
    "# Load formatted data\n",
    "dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust based on your needs\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wikipedia_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,     # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=16,     # Effective batch size = 128\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                         # Mixed precision training\n",
    "    save_steps=100,                    # Save every 100 steps\n",
    "    eval_steps=100,                    # Evaluate every 100 steps\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,                # Keep only last 3 checkpoints (saves disk space)\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",          # Memory efficient optimizer\n",
    "    resume_from_checkpoint=True,       # Auto-resume from latest checkpoint\n",
    "    gradient_checkpointing=True,       # Enable gradient checkpointing for memory efficiency\n",
    "    dataloader_num_workers=2,          # Use 2 workers for data loading\n",
    "    logging_dir=\"./logs\",              # Directory for TensorBoard logs\n",
    "    report_to=[\"tensorboard\"],         # Enable TensorBoard logging\n",
    "    seed=42,                           # Set seed for reproducibility\n",
    "    weight_decay=0.01,                 # Add weight decay for regularization\n",
    "    max_grad_norm=1.0,                 # Gradient clipping\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory\"\"\"\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    latest = checkpoints[-1]\n",
    "    return latest\n",
    "\n",
    "# Look for existing checkpoint\n",
    "checkpoint_path = find_latest_checkpoint(\"./wikipedia_model\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "    print(f\"Progress: Step {checkpoint_path.split('-')[-1]}\")\n",
    "    \n",
    "    # Resume training from checkpoint\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found. Starting fresh training...\")\n",
    "    # Start training from scratch\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# Set model to eval mode\n",
    "# model.eval() ## training sets it to eval automatically\n",
    "\n",
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./wikipedia_model/final\")\n",
    "tokenizer.save_pretrained(\"./wikipedia_model/final\")\n",
    "# Also save the LoRA weights explicitly\n",
    "model.save_pretrained(\"./wikipedia_model/final\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total training time: {training_duration:.2f} seconds ({training_duration/3600:.2f} hours)\")\n",
    "print(f\"Model saved to: ./wikipedia_model/final\")\n",
    "print(f\"TensorBoard logs: ./logs (run 'tensorboard --logdir ./logs' to view)\")\n",
    "print(f\"Checkpoints saved every {training_args.save_steps} steps\")\n",
    "print(f\"Final step: {trainer.state.global_step}\")\n",
    "print(f\"Final epoch: {trainer.state.epoch:.2f}\")\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e73f1",
   "metadata": {},
   "source": [
    "## VRAM Cleanup\n",
    "\n",
    "Clears GPU memory cache and forces garbage collection to free up VRAM after training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5002ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_vram():\n",
    "    \"\"\"Clear VRAM completely\"\"\"\n",
    "    print(\"Clearing VRAM...\")\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"VRAM allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"VRAM reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(\"VRAM cleared!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99203de5",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Loads a trained checkpoint and tests the model's ability to generate informative responses about various topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text, max_length=512):\n",
    "    \"\"\"Calculate perplexity for a given text\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_rouge(generated, reference):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores\"\"\"\n",
    "    # BLEU\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothing)\n",
    "    \n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, generated)\n",
    "    \n",
    "    return bleu, rouge_scores\n",
    "\n",
    "def load_checkpoint_model(checkpoint_path):\n",
    "    \"\"\"Load model from checkpoint\"\"\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_prompts):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    results = {\n",
    "        'perplexity': [],\n",
    "        'bleu': [],\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': [],\n",
    "        'responses': []\n",
    "    }\n",
    "    \n",
    "    for prompt_data in test_prompts:\n",
    "        prompt = prompt_data['prompt']\n",
    "        reference = prompt_data['reference']\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=len(inputs['input_ids'][0]) + 100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answer = response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        perplexity = calculate_perplexity(model, tokenizer, generated_answer)\n",
    "        bleu, rouge_scores = calculate_bleu_rouge(generated_answer, reference)\n",
    "        \n",
    "        results['perplexity'].append(perplexity)\n",
    "        results['bleu'].append(bleu)\n",
    "        results['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "        results['responses'].append({\n",
    "            'prompt': prompt,\n",
    "            'generated': generated_answer,\n",
    "            'reference': reference\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    for key in ['perplexity', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "        results[f'avg_{key}'] = np.mean(results[key])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts with references for evaluation\n",
    "test_prompts = [\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nProvide information about Python programming language.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Python is a high-level programming language known for its simplicity and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nExplain what Artificial Intelligence is.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. It includes learning, reasoning, and self-correction.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nTell me about Machine Learning.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Machine Learning is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\\n\",\n",
    "        'reference': \"The capital of France is Paris, which is located in the north-central part of the country along the Seine River.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nDescribe the process of photosynthesis.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Photosynthesis is the process by which plants convert light energy into chemical energy. It involves chlorophyll absorbing sunlight and using it to convert carbon dioxide and water into glucose and oxygen.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Find all checkpoints for comparison\n",
    "checkpoints = glob.glob(\"./wikipedia_model/checkpoint-*\")\n",
    "checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"Available checkpoints:\")\n",
    "    for i, cp in enumerate(checkpoints):\n",
    "        print(f\"{i+1}. {cp}\")\n",
    "    \n",
    "    # Test latest checkpoint\n",
    "    latest_checkpoint = checkpoints[-1]\n",
    "    print(f\"\\nTesting latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    model, tokenizer = load_checkpoint_model(latest_checkpoint)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_model(model, tokenizer, test_prompts)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average Perplexity: {results['avg_perplexity']:.2f}\")\n",
    "    print(f\"Average BLEU Score: {results['avg_bleu']:.4f}\")\n",
    "    print(f\"Average ROUGE-1 F1: {results['avg_rouge1']:.4f}\")\n",
    "    print(f\"Average ROUGE-2 F1: {results['avg_rouge2']:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1: {results['avg_rougeL']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define markers for splitting\n",
    "    instr_marker = '### Instruction:\\n'\n",
    "    resp_marker = '\\n\\n### Response:\\n'\n",
    "    \n",
    "    # Show sample responses\n",
    "    print(\"\\nSample Responses:\")\n",
    "    for i, resp in enumerate(results['responses'][:3]):\n",
    "        print(f\"\\n{i+1}. Prompt: {resp['prompt'].split(instr_marker)[1].split(resp_marker)[0]}\")\n",
    "        print(f\"   Generated: {resp['generated'][:100]}...\")\n",
    "        print(f\"   Reference: {resp['reference'][:100]}...\")\n",
    "    \n",
    "    # Checkpoint comparison (if multiple checkpoints exist)\n",
    "    if len(checkpoints) > 1:\n",
    "        print(f\"\\nCheckpoint Comparison (last {min(3, len(checkpoints))} checkpoints):\")\n",
    "        comparison_results = []\n",
    "        \n",
    "        for cp in checkpoints[-3:]:\n",
    "            try:\n",
    "                model_comp, tokenizer_comp = load_checkpoint_model(cp)\n",
    "                results_comp = evaluate_model(model_comp, tokenizer_comp, test_prompts[:2])  # Quick comparison\n",
    "                step = cp.split(\"-\")[-1]\n",
    "                comparison_results.append({\n",
    "                    'step': step,\n",
    "                    'perplexity': results_comp['avg_perplexity'],\n",
    "                    'bleu': results_comp['avg_bleu']\n",
    "                })\n",
    "                del model_comp\n",
    "                torch.cuda.empty_cache()\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for res in comparison_results:\n",
    "            print(f\"Step {res['step']}: Perplexity={res['perplexity']:.2f}, BLEU={res['bleu']:.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoints found. Please train the model first.\")\n",
    "\n",
    "print(\"\\nModel testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668de446",
   "metadata": {},
   "source": [
    "## Chat Interface\n",
    "\n",
    "Creates an interactive chat interface for real-time conversation with the trained Wikipedia model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class OrganizedChatInterface:\n",
    "    def __init__(self, model_path=\"./wikipedia_model/final\"):\n",
    "        print(\"Loading organized model and tokenizer...\")\n",
    "        try:\n",
    "            # Load base model with simpler device mapping for speed\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"microsoft/phi-2\",\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map={\"\": 0} if torch.cuda.is_available() else {\"\": \"cpu\"},  # Simpler mapping\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            # Load PEFT adapter\n",
    "            self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            print(\"Organized model loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Conversation history\n",
    "        self.history = []\n",
    "        self.max_history = 10\n",
    "\n",
    "    def organize_response(self, response, query):\n",
    "        \"\"\"Organize and clean up the response to sound more coherent\"\"\"\n",
    "        # Remove the original query from the response\n",
    "        if response.lower().startswith(query.lower()):\n",
    "            response = response[len(query):].strip()\n",
    "\n",
    "        # Clean up common Wikipedia artifacts\n",
    "        response = re.sub(r'\\s+', ' ', response)  # Multiple spaces to single\n",
    "        response = re.sub(r'\\n+', ' ', response)  # Newlines to spaces\n",
    "\n",
    "        # Split into sentences and clean up\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', response.strip())\n",
    "\n",
    "        # Filter and organize sentences\n",
    "        organized_sentences = []\n",
    "        for sentence in sentences[:5]:  # Limit to 5 sentences\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10 and not sentence.startswith(('See also', 'References', 'External links', 'Category:')):\n",
    "                # Capitalize first letter\n",
    "                if sentence:\n",
    "                    sentence = sentence[0].upper() + sentence[1:]\n",
    "                organized_sentences.append(sentence)\n",
    "\n",
    "        # Join with proper punctuation\n",
    "        organized_response = '. '.join(organized_sentences)\n",
    "        if organized_response and not organized_response.endswith(('.', '!', '?')):\n",
    "            organized_response += '.'\n",
    "\n",
    "        return organized_response.strip()\n",
    "\n",
    "    def generate_organized_response(self, query, max_length=300, temperature=0.8, top_p=0.9):\n",
    "        \"\"\"Generate organized, coherent response\"\"\"\n",
    "        try:\n",
    "            # Create a better prompt that works with Wikipedia training data\n",
    "            # Guide the model toward encyclopedic explanations\n",
    "            if query.lower().startswith(('what is', 'who is', 'where is', 'when', 'how', 'why')):\n",
    "                # For questions, create a prompt that leads to explanations\n",
    "                prompt = f\"{query[0].upper()}{query[1:]} \"\n",
    "            elif query.lower().startswith(('tell me about', 'explain', 'describe')):\n",
    "                # For explanation requests\n",
    "                prompt = f\"{query[0].upper()}{query[1:]} \"\n",
    "            else:\n",
    "                # For general queries, assume it's asking for information\n",
    "                prompt = f\"{query} is \"\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length + len(inputs['input_ids'][0]),\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2,  # Add some repetition penalty for coherence\n",
    "                    no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
    "                    num_beams=1\n",
    "                )\n",
    "\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Organize and clean the response\n",
    "            organized_response = self.organize_response(full_response, prompt.strip())\n",
    "\n",
    "            # If response is too short or empty, try a different approach\n",
    "            if len(organized_response) < 20:\n",
    "                # Fallback: try with \"is\" prefix\n",
    "                fallback_prompt = f\"{query} is\"\n",
    "                inputs2 = self.tokenizer(fallback_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs2 = {k: v.cuda() for k, v in inputs2.items()}\n",
    "\n",
    "                outputs2 = self.model.generate(\n",
    "                    **inputs2,\n",
    "                    max_length=max_length + len(inputs2['input_ids'][0]),\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    num_beams=1\n",
    "                )\n",
    "\n",
    "                fallback_response = self.tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "                organized_response = self.organize_response(fallback_response, fallback_prompt)\n",
    "\n",
    "            # Add to history\n",
    "            self.history.append((query, organized_response))\n",
    "            if len(self.history) > self.max_history:\n",
    "                self.history.pop(0)\n",
    "\n",
    "            return organized_response if organized_response else f\"I don't have enough information about '{query}' in my training data.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def interactive_chat(self):\n",
    "        \"\"\"Interactive chat with organized responses\"\"\"\n",
    "        print(\"ORGANIZED WIKIPEDIA LLM CHAT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"This model provides organized, coherent responses from Wikipedia knowledge.\")\n",
    "        print(\"Type 'quit' to exit, 'history' to view chat history, 'clear' to reset\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nYou: \").strip()\n",
    "\n",
    "                if query.lower() == 'quit':\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                elif query.lower() == 'history':\n",
    "                    print(\"\\nChat History:\")\n",
    "                    for i, (q, r) in enumerate(self.history[-5:], 1):\n",
    "                        print(f\"{i}. Q: {q}\")\n",
    "                        print(f\"   A: {r[:150]}...\")\n",
    "                    continue\n",
    "                elif query.lower() == 'clear':\n",
    "                    self.history.clear()\n",
    "                    print(\"History cleared.\")\n",
    "                    continue\n",
    "\n",
    "                if not query:\n",
    "                    continue\n",
    "\n",
    "                print(\"Assistant: \", end=\"\", flush=True)\n",
    "                response = self.generate_organized_response(query)\n",
    "                print(response)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nInterrupted by user.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.history.clear()\n",
    "        print(\"Conversation history cleared.\")\n",
    "\n",
    "# Create the organized interface\n",
    "organized_chat = OrganizedChatInterface()\n",
    "\n",
    "# Start interactive chat\n",
    "organized_chat.interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b989c",
   "metadata": {},
   "source": [
    "## Training Restart\n",
    "\n",
    "Safely deletes all training checkpoints and logs to allow restarting the training process from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def restart_training():\n",
    "    \"\"\"Delete all training files to restart from scratch\"\"\"\n",
    "    \n",
    "    # Directories to delete\n",
    "    dirs_to_delete = [\"./wikipedia_model\", \"./logs\"]\n",
    "    \n",
    "    for dir_path in dirs_to_delete:\n",
    "        if os.path.exists(dir_path):\n",
    "            print(f\"Deleting {dir_path}...\")\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Deleted {dir_path}\")\n",
    "        else:\n",
    "            print(f\"{dir_path} does not exist\")\n",
    "    \n",
    "    print(\"\\nAll training files deleted. You can now run the training cell to start fresh.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"This will delete all training checkpoints and logs. Are you sure? (yes/no): \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        restart_training()\n",
    "    else:\n",
    "        print(\"Restart cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b794f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Automated hyperparameter optimization using grid search to find the best learning rate, batch size, and LoRA parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_OPTIONS = {\n",
    "    \"small\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B - easiest to train\n",
    "    \"medium\": \"microsoft/phi-2\",  # 2.7B - good balance\n",
    "    \"large\": \"mistralai/Mistral-7B-v0.1\"  # 7B - needs quantization\n",
    "}\n",
    "\n",
    "model_name = MODEL_OPTIONS[\"medium\"]  # Start with Phi-2\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for hyperparameter evaluation\"\"\"\n",
    "    \n",
    "    learning_rate, per_device_batch_size, lora_rank, lora_alpha, weight_decay = params\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # For causal LM, labels are the same as input_ids\n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    tokenized_train = dataset[\"train\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    tokenized_test = dataset[\"test\"].select(range(200)).map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    # Calculate effective batch size\n",
    "    gradient_accumulation_steps = max(1, 16 // per_device_batch_size)  # Target effective batch of 16\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./grid_trials\",\n",
    "        num_train_epochs=1,  # Quick trial\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=True,\n",
    "        save_steps=50,\n",
    "        eval_steps=50,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        seed=42,\n",
    "        report_to=[],  # Disable integrations to avoid MLflow issues\n",
    "        remove_unused_columns=False,  # Keep all columns\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA with parameters\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Load model (assuming model and tokenizer are already loaded in notebook)\n",
    "    model_with_lora = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_with_lora,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Get best eval loss\n",
    "    best_eval_loss = trainer.state.best_metric\n",
    "    \n",
    "    # Cleanup\n",
    "    del model_with_lora\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_eval_loss, params\n",
    "\n",
    "def run_hyperparameter_tuning():\n",
    "    \"\"\"Run grid search hyperparameter optimization\"\"\"\n",
    "    print(\"Starting hyperparameter tuning with grid search...\")\n",
    "    print(\"This may take several hours depending on the number of combinations.\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "        'per_device_batch_size': [4, 8],\n",
    "        'lora_rank': [8, 16],\n",
    "        'lora_alpha': [16, 32],\n",
    "        'weight_decay': [0.01, 0.05]\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    combinations = list(itertools.product(*values))\n",
    "    \n",
    "    print(f\"Testing {len(combinations)} parameter combinations...\")\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    for i, combo in enumerate(tqdm(combinations, desc=\"Hyperparameter Tuning\", unit=\"trial\")):\n",
    "        params = dict(zip(keys, combo))\n",
    "        print(f\"\\nTrial {i+1}/{len(combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            loss, _ = objective(combo)\n",
    "            print(f\"Eval loss: {loss:.4f}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = params\n",
    "                print(f\"New best loss: {best_loss:.4f} (Trial {i+1})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best eval loss: {best_loss:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    with open(\"best_hyperparams.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nBest parameters saved to best_hyperparams.json\")\n",
    "    print(\"Use these parameters in your main training script.\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Uncomment to run tuning (takes significant time and resources)\n",
    "if __name__ == \"__main__\":\n",
    "    best_params = run_hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0332f",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Comprehensive model evaluation with multiple metrics including BLEU, ROUGE, perplexity, and custom benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "class RobustModelEvaluator:\n",
    "    \"\"\"Robust and comprehensive model evaluation suite with extensive error handling\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str = \"./wikipedia_model/final\", device: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with robust error handling.\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to the model directory\n",
    "            device: Device to use ('auto', 'cuda', 'cpu', or specific device number)\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.generator = None\n",
    "        self.rouge_scorer = None\n",
    "        self.smooth = None\n",
    "\n",
    "        print(f\"Initializing robust model evaluator for {model_path}...\")\n",
    "\n",
    "        try:\n",
    "            self._load_model()\n",
    "            self._setup_tokenizer()\n",
    "            self._setup_pipeline()\n",
    "            self._setup_scorers()\n",
    "            print(\"✓ Model evaluator initialized successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to initialize evaluator: {e}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the model with comprehensive error handling\"\"\"\n",
    "        print(\"Loading model...\")\n",
    "\n",
    "        try:\n",
    "            # Determine device mapping\n",
    "            if self.device == \"auto\":\n",
    "                if torch.cuda.is_available():\n",
    "                    device_map = {\"\": 0}  # Use first GPU\n",
    "                    torch_dtype = torch.float16\n",
    "                else:\n",
    "                    device_map = {\"\": \"cpu\"}\n",
    "                    torch_dtype = torch.float32\n",
    "            elif self.device == \"cpu\":\n",
    "                device_map = {\"\": \"cpu\"}\n",
    "                torch_dtype = torch.float32\n",
    "            else:\n",
    "                # Specific GPU device\n",
    "                device_num = int(self.device) if isinstance(self.device, str) and self.device.isdigit() else 0\n",
    "                device_map = {\"\": device_num}\n",
    "                torch_dtype = torch.float16\n",
    "\n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"microsoft/phi-2\",\n",
    "                torch_dtype=torch_dtype,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "\n",
    "            # Load PEFT adapter\n",
    "            self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "\n",
    "            print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_tokenizer(self):\n",
    "        \"\"\"Setup tokenizer with proper configuration\"\"\"\n",
    "        print(\"Setting up tokenizer...\")\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "            # Ensure pad token is set\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Ensure we have the necessary special tokens\n",
    "            if self.tokenizer.eos_token is None:\n",
    "                self.tokenizer.eos_token = \"</s>\"\n",
    "\n",
    "            print(\"✓ Tokenizer configured successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error setting up tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_pipeline(self):\n",
    "        \"\"\"Setup text generation pipeline with device handling\"\"\"\n",
    "        print(\"Setting up text generation pipeline...\")\n",
    "\n",
    "        try:\n",
    "            pipeline_kwargs = {\n",
    "                \"model\": self.model,\n",
    "                \"tokenizer\": self.tokenizer,\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.7,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            # Only specify device if not using accelerate device_map\n",
    "            if not hasattr(self.model, 'hf_device_map') or self.model.hf_device_map is None:\n",
    "                if torch.cuda.is_available():\n",
    "                    pipeline_kwargs[\"device\"] = 0\n",
    "                else:\n",
    "                    pipeline_kwargs[\"device\"] = -1\n",
    "\n",
    "            self.generator = pipeline(\"text-generation\", **pipeline_kwargs)\n",
    "            print(\"✓ Pipeline configured successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error setting up pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_scorers(self):\n",
    "        \"\"\"Setup evaluation scorers\"\"\"\n",
    "        try:\n",
    "            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "            self.smooth = SmoothingFunction().method4\n",
    "            print(\"✓ Scorers configured successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error setting up scorers: {e}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_perplexity(self, texts: List[str], batch_size: int = 4) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity with robust error handling.\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings to evaluate\n",
    "            batch_size: Batch size for processing\n",
    "\n",
    "        Returns:\n",
    "            Average perplexity score\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            print(\"Warning: No texts provided for perplexity calculation\")\n",
    "            return float('inf')\n",
    "\n",
    "        perplexities = []\n",
    "        valid_texts = []\n",
    "\n",
    "        # Filter and validate texts\n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and len(text.strip()) > 0:\n",
    "                valid_texts.append(text.strip())\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid text: {type(text)}\")\n",
    "\n",
    "        if not valid_texts:\n",
    "            print(\"Warning: No valid texts for perplexity calculation\")\n",
    "            return float('inf')\n",
    "\n",
    "        print(f\"Calculating perplexity for {len(valid_texts)} texts...\")\n",
    "\n",
    "        try:\n",
    "            for i in range(0, len(valid_texts), batch_size):\n",
    "                batch_texts = valid_texts[i:i+batch_size]\n",
    "\n",
    "                try:\n",
    "                    # Tokenize with error handling\n",
    "                    encodings = self.tokenizer(\n",
    "                        batch_texts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512  # Limit length to avoid memory issues\n",
    "                    )\n",
    "\n",
    "                    # Move to appropriate device\n",
    "                    device = next(self.model.parameters()).device\n",
    "                    input_ids = encodings.input_ids.to(device)\n",
    "                    attention_mask = encodings.attention_mask.to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=input_ids\n",
    "                        )\n",
    "\n",
    "                        if outputs.loss is not None:\n",
    "                            loss = outputs.loss.item()\n",
    "                            if not np.isinf(loss) and not np.isnan(loss):\n",
    "                                perplexity = np.exp(loss)\n",
    "                                if not np.isinf(perplexity) and not np.isnan(perplexity):\n",
    "                                    perplexities.append(perplexity)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing batch {i//batch_size}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if perplexities:\n",
    "                avg_perplexity = np.mean(perplexities)\n",
    "                print(f\"Average perplexity: {avg_perplexity:.4f}\")\n",
    "                return avg_perplexity\n",
    "            else:\n",
    "                print(\"Warning: No valid perplexity calculations\")\n",
    "                return float('inf')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in perplexity calculation: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def _safe_extract_generated_text(self, full_text: str, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Safely extract generated text from full pipeline output.\n",
    "\n",
    "        Args:\n",
    "            full_text: Complete text returned by pipeline\n",
    "            prompt: Original prompt\n",
    "\n",
    "        Returns:\n",
    "            Extracted generated text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(full_text, str) or not isinstance(prompt, str):\n",
    "                return \"\"\n",
    "\n",
    "            # Remove prompt from the beginning if present\n",
    "            if full_text.startswith(prompt):\n",
    "                generated = full_text[len(prompt):].strip()\n",
    "            else:\n",
    "                generated = full_text.strip()\n",
    "\n",
    "            # Clean up common artifacts\n",
    "            generated = generated.split('\\n\\n')[0]  # Take only first paragraph\n",
    "            generated = generated.split('\\n')[0]    # Take only first line if multi-line\n",
    "\n",
    "            return generated\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error extracting generated text: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def calculate_bleu_rouge(self, generated_texts: List[str], reference_texts: List[str],\n",
    "                           prompts: Optional[List[str]] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate BLEU and ROUGE scores with comprehensive error handling.\n",
    "\n",
    "        Args:\n",
    "            generated_texts: List of generated text strings\n",
    "            reference_texts: List of reference text strings\n",
    "            prompts: Optional list of prompts used for generation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with BLEU and ROUGE scores\n",
    "        \"\"\"\n",
    "        if not generated_texts or not reference_texts:\n",
    "            print(\"Warning: Empty text lists provided\")\n",
    "            return {'bleu': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "\n",
    "        bleu_scores = []\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "\n",
    "        min_length = min(len(generated_texts), len(reference_texts))\n",
    "        print(f\"Calculating BLEU/ROUGE for {min_length} text pairs...\")\n",
    "\n",
    "        for i in range(min_length):\n",
    "            try:\n",
    "                gen_raw = generated_texts[i] if i < len(generated_texts) else \"\"\n",
    "                ref_raw = reference_texts[i] if i < len(reference_texts) else \"\"\n",
    "                prompt = prompts[i] if prompts and i < len(prompts) else \"\"\n",
    "\n",
    "                # Extract clean generated text\n",
    "                gen_clean = self._safe_extract_generated_text(gen_raw, prompt)\n",
    "\n",
    "                # Validate inputs\n",
    "                if not isinstance(gen_clean, str) or not isinstance(ref_raw, str):\n",
    "                    print(f\"Warning: Non-string inputs at index {i}\")\n",
    "                    continue\n",
    "\n",
    "                gen_clean = gen_clean.strip()\n",
    "                ref_clean = ref_raw.strip()\n",
    "\n",
    "                if len(gen_clean) == 0 or len(ref_clean) == 0:\n",
    "                    continue\n",
    "\n",
    "                # BLEU calculation\n",
    "                try:\n",
    "                    gen_tokens = gen_clean.split()\n",
    "                    ref_tokens = ref_clean.split()\n",
    "\n",
    "                    if len(gen_tokens) > 0 and len(ref_tokens) > 0:\n",
    "                        bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=self.smooth)\n",
    "                        if not np.isnan(bleu):\n",
    "                            bleu_scores.append(bleu)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: BLEU calculation failed for sample {i}: {e}\")\n",
    "\n",
    "                # ROUGE calculation\n",
    "                try:\n",
    "                    rouge_scores = self.rouge_scorer.score(ref_clean, gen_clean)\n",
    "                    if hasattr(rouge_scores, 'get') and 'rouge1' in rouge_scores:\n",
    "                        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "                        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "                        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: ROUGE calculation failed for sample {i}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: General error processing sample {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate averages\n",
    "        result = {\n",
    "            'bleu': np.mean(bleu_scores) if bleu_scores else 0.0,\n",
    "            'rouge1': np.mean(rouge1_scores) if rouge1_scores else 0.0,\n",
    "            'rouge2': np.mean(rouge2_scores) if rouge2_scores else 0.0,\n",
    "            'rougeL': np.mean(rougeL_scores) if rougeL_scores else 0.0\n",
    "        }\n",
    "\n",
    "        print(f\"✓ Calculated scores - BLEU: {result['bleu']:.4f}, ROUGE-1: {result['rouge1']:.4f}\")\n",
    "        return result\n",
    "\n",
    "    def generate_samples(self, prompts: List[str], num_samples: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generate text samples with robust error handling.\n",
    "\n",
    "        Args:\n",
    "            prompts: List of prompt strings\n",
    "            num_samples: Number of samples to generate\n",
    "\n",
    "        Returns:\n",
    "            List of sample dictionaries\n",
    "        \"\"\"\n",
    "        if not prompts:\n",
    "            print(\"Warning: No prompts provided\")\n",
    "            return []\n",
    "\n",
    "        samples = []\n",
    "        valid_prompts = [p for p in prompts if isinstance(p, str) and len(p.strip()) > 0][:num_samples]\n",
    "\n",
    "        if not valid_prompts:\n",
    "            print(\"Warning: No valid prompts found\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Generating {len(valid_prompts)} text samples...\")\n",
    "\n",
    "        for i, prompt in enumerate(tqdm(valid_prompts, desc=\"Generating samples\")):\n",
    "            try:\n",
    "                # Generate with timeout and error handling\n",
    "                outputs = self.generator(\n",
    "                    prompt,\n",
    "                    max_new_tokens=100,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "                # Extract generated text safely\n",
    "                if isinstance(outputs, list) and len(outputs) > 0:\n",
    "                    if isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
    "                        generated_text = outputs[0]['generated_text']\n",
    "                    elif isinstance(outputs[0], str):\n",
    "                        generated_text = outputs[0]\n",
    "                    else:\n",
    "                        generated_text = str(outputs[0])\n",
    "                else:\n",
    "                    generated_text = str(outputs)\n",
    "\n",
    "                # Clean the generated text\n",
    "                clean_generated = self._safe_extract_generated_text(generated_text, prompt)\n",
    "\n",
    "                samples.append({\n",
    "                    'prompt': prompt,\n",
    "                    'generated': generated_text,  # Keep full text\n",
    "                    'generated_clean': clean_generated,  # Clean version for evaluation\n",
    "                    'length': len(clean_generated.split()) if clean_generated else 0\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error generating for prompt {i}: {e}\")\n",
    "                samples.append({\n",
    "                    'prompt': prompt,\n",
    "                    'generated': \"Error: Failed to generate text\",\n",
    "                    'generated_clean': \"\",\n",
    "                    'length': 0\n",
    "                })\n",
    "\n",
    "        print(f\"✓ Generated {len(samples)} samples successfully\")\n",
    "        return samples\n",
    "\n",
    "    def benchmark_model(self, test_dataset=None, num_samples: int = 50) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Run comprehensive benchmark with robust error handling.\n",
    "\n",
    "        Args:\n",
    "            test_dataset: Optional dataset to use for evaluation\n",
    "            num_samples: Number of samples to evaluate\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation results or None if failed\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STARTING COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        try:\n",
    "            # Load test dataset\n",
    "            if test_dataset is None:\n",
    "                print(\"Loading test dataset...\")\n",
    "                try:\n",
    "                    dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "                    test_dataset = dataset[\"test\"]\n",
    "                    print(\"✓ Dataset loaded successfully\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error loading dataset: {e}\")\n",
    "                    print(\"Please ensure the dataset is available at 'data/formatted_wikipedia'\")\n",
    "                    return None\n",
    "\n",
    "            # Sample test data\n",
    "            available_samples = len(test_dataset)\n",
    "            actual_samples = min(num_samples, available_samples)\n",
    "            print(f\"Using {actual_samples} samples out of {available_samples} available\")\n",
    "\n",
    "            test_items = test_dataset.select(range(actual_samples))\n",
    "\n",
    "            # Extract texts and create prompts\n",
    "            test_texts = []\n",
    "            test_prompts = []\n",
    "\n",
    "            for item in test_items:\n",
    "                try:\n",
    "                    text = item['text']\n",
    "                    if isinstance(text, str) and len(text.strip()) > 0:\n",
    "                        test_texts.append(text.strip())\n",
    "                        # Create prompt from first 200 characters\n",
    "                        prompt = text[:200].strip()\n",
    "                        if not prompt.endswith(('...', '.', '!', '?')):\n",
    "                            prompt += \"...\"\n",
    "                        test_prompts.append(prompt)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing dataset item: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not test_texts or not test_prompts:\n",
    "                print(\"✗ No valid test data found\")\n",
    "                return None\n",
    "\n",
    "            # Calculate perplexity\n",
    "            print(\"\\nCalculating perplexity...\")\n",
    "            perplexity = self.calculate_perplexity(test_texts)\n",
    "\n",
    "            # Generate samples\n",
    "            print(\"\\nGenerating samples for BLEU/ROUGE evaluation...\")\n",
    "            generated_samples = self.generate_samples(test_prompts, num_samples=len(test_prompts))\n",
    "\n",
    "            if not generated_samples:\n",
    "                print(\"✗ No samples generated\")\n",
    "                return None\n",
    "\n",
    "            # Extract texts for evaluation\n",
    "            generated_texts = [sample['generated'] for sample in generated_samples]\n",
    "            reference_texts = test_texts[:len(generated_samples)]\n",
    "\n",
    "            # Calculate BLEU and ROUGE\n",
    "            print(\"\\nCalculating BLEU and ROUGE scores...\")\n",
    "            text_metrics = self.calculate_bleu_rouge(generated_texts, reference_texts, test_prompts)\n",
    "\n",
    "            # Compile results\n",
    "            results = {\n",
    "                'perplexity': float(perplexity) if not np.isinf(perplexity) else 999.0,\n",
    "                'bleu_score': float(text_metrics['bleu']),\n",
    "                'rouge1_score': float(text_metrics['rouge1']),\n",
    "                'rouge2_score': float(text_metrics['rouge2']),\n",
    "                'rougeL_score': float(text_metrics['rougeL']),\n",
    "                'samples': generated_samples[:5],  # Save first 5 samples\n",
    "                'num_samples_evaluated': len(generated_samples),\n",
    "                'evaluation_timestamp': str(torch.randint(0, 1000000, (1,)).item())  # Simple timestamp\n",
    "            }\n",
    "\n",
    "            # Print results\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"MODEL EVALUATION RESULTS\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Perplexity: {results['perplexity']:.4f}\")\n",
    "            print(f\"BLEU Score: {results['bleu_score']:.4f}\")\n",
    "            print(f\"ROUGE-1 Score: {results['rouge1_score']:.4f}\")\n",
    "            print(f\"ROUGE-2 Score: {results['rouge2_score']:.4f}\")\n",
    "            print(f\"ROUGE-L Score: {results['rougeL_score']:.4f}\")\n",
    "            print(f\"Samples evaluated: {results['num_samples_evaluated']}\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            # Save results\n",
    "            try:\n",
    "                with open(\"evaluation_results.json\", \"w\", encoding='utf-8') as f:\n",
    "                    # Create JSON-serializable version\n",
    "                    json_results = {\n",
    "                        'perplexity': results['perplexity'],\n",
    "                        'bleu_score': results['bleu_score'],\n",
    "                        'rouge1_score': results['rouge1_score'],\n",
    "                        'rouge2_score': results['rouge2_score'],\n",
    "                        'rougeL_score': results['rougeL_score'],\n",
    "                        'num_samples_evaluated': results['num_samples_evaluated'],\n",
    "                        'evaluation_timestamp': results['evaluation_timestamp'],\n",
    "                        'samples': results['samples']\n",
    "                    }\n",
    "                    json.dump(json_results, f, indent=2, ensure_ascii=False)\n",
    "                print(\"✓ Results saved to evaluation_results.json\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error saving results: {e}\")\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in benchmark: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def plot_evaluation_history(self, checkpoint_dirs=None):\n",
    "        \"\"\"Plot evaluation metrics across checkpoints\"\"\"\n",
    "        try:\n",
    "            if checkpoint_dirs is None:\n",
    "                checkpoint_dirs = [d for d in os.listdir(\"./wikipedia_model\")\n",
    "                                 if d.startswith(\"checkpoint-\") and os.path.isdir(f\"./wikipedia_model/{d}\")]\n",
    "                checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[1]) if x.split(\"-\")[1].isdigit() else 0)\n",
    "\n",
    "            perplexities = []\n",
    "            steps = []\n",
    "\n",
    "            for checkpoint in checkpoint_dirs:\n",
    "                checkpoint_path = f\"./wikipedia_model/{checkpoint}\"\n",
    "                results_file = f\"{checkpoint_path}/evaluation_results.json\"\n",
    "\n",
    "                if os.path.exists(results_file):\n",
    "                    try:\n",
    "                        with open(results_file, \"r\", encoding='utf-8') as f:\n",
    "                            data = json.load(f)\n",
    "                            perplexity = data.get('perplexity', 0)\n",
    "                            if isinstance(perplexity, (int, float)) and not np.isinf(perplexity):\n",
    "                                perplexities.append(perplexity)\n",
    "                                step = int(checkpoint.split(\"-\")[1]) if checkpoint.split(\"-\")[1].isdigit() else 0\n",
    "                                steps.append(step)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error reading {results_file}: {e}\")\n",
    "\n",
    "            if perplexities and steps:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.plot(steps, perplexities, marker='o', linewidth=2, markersize=8)\n",
    "                plt.xlabel('Training Steps', fontsize=12)\n",
    "                plt.ylabel('Perplexity', fontsize=12)\n",
    "                plt.title('Model Perplexity Across Training Checkpoints', fontsize=14, fontweight='bold')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('evaluation_history.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                print(\"✓ Evaluation history plot saved as 'evaluation_history.png'\")\n",
    "            else:\n",
    "                print(\"No valid evaluation results found in checkpoints\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting evaluation history: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def run_robust_evaluation(model_path: str = \"./wikipedia_model/final\",\n",
    "                         num_samples: int = 50) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run robust model evaluation with comprehensive error handling.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        num_samples: Number of samples to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Evaluation results dictionary or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting robust evaluation of model: {model_path}\")\n",
    "        evaluator = RobustModelEvaluator(model_path)\n",
    "        results = evaluator.benchmark_model(num_samples=num_samples)\n",
    "\n",
    "        if results:\n",
    "            # Plot evaluation history\n",
    "            evaluator.plot_evaluation_history()\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Evaluation failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Uncomment to run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_robust_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bf12b",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Analyze your training dataset statistics, distribution, and quality metrics to understand your model's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import os\n",
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"Comprehensive dataset analysis tools\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path=\"data/formatted_wikipedia\"):\n",
    "        try:\n",
    "            self.dataset = load_from_disk(dataset_path)\n",
    "            print(f\"Loaded dataset with {len(self.dataset)} splits\")\n",
    "            \n",
    "            # Debug: Check dataset structure\n",
    "            if len(self.dataset) > 0:\n",
    "                first_split = list(self.dataset.keys())[0]\n",
    "                first_item = self.dataset[first_split][0]\n",
    "                print(f\"First item type: {type(first_item)}\")\n",
    "                if isinstance(first_item, dict):\n",
    "                    print(f\"First item keys: {list(first_item.keys())}\")\n",
    "                else:\n",
    "                    print(f\"First item (first 100 chars): {str(first_item)[:100]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            self.dataset = None\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Calculate basic dataset statistics\"\"\"\n",
    "        if self.dataset is None:\n",
    "            return None\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        for split_name, split_data in self.dataset.items():\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in split_data]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in split_data if isinstance(item, str)]\n",
    "            \n",
    "            # Text lengths\n",
    "            text_lengths = [len(text.split()) for text in texts]\n",
    "            \n",
    "            stats[split_name] = {\n",
    "                'num_samples': len(texts),\n",
    "                'avg_words': np.mean(text_lengths),\n",
    "                'min_words': np.min(text_lengths),\n",
    "                'max_words': np.max(text_lengths),\n",
    "                'median_words': np.median(text_lengths),\n",
    "                'total_words': sum(text_lengths),\n",
    "                'avg_chars': np.mean([len(text) for text in texts])\n",
    "            }\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for split, stat in stats.items():\n",
    "            print(f\"\\n{split.upper()} SPLIT:\")\n",
    "            print(f\"  Samples: {stat['num_samples']:,}\")\n",
    "            print(f\"  Avg words per sample: {stat['avg_words']:.1f}\")\n",
    "            print(f\"  Word count range: {stat['min_words']}-{stat['max_words']}\")\n",
    "            print(f\"  Median words: {stat['median_words']:.1f}\")\n",
    "            print(f\"  Total words: {stat['total_words']:,}\")\n",
    "            print(f\"  Avg characters per sample: {stat['avg_chars']:.1f}\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_distributions(self, stats):\n",
    "        \"\"\"Plot data distributions\"\"\"\n",
    "        if stats is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Dataset Analysis', fontsize=16)\n",
    "        \n",
    "        splits = list(stats.keys())\n",
    "        colors = ['blue', 'green', 'red', 'orange']\n",
    "        \n",
    "        # Sample counts\n",
    "        axes[0, 0].bar(splits, [stats[s]['num_samples'] for s in splits], color=colors[:len(splits)])\n",
    "        axes[0, 0].set_title('Number of Samples per Split')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Average words\n",
    "        axes[0, 1].bar(splits, [stats[s]['avg_words'] for s in splits], color=colors[:len(splits)])\n",
    "        axes[0, 1].set_title('Average Words per Sample')\n",
    "        axes[0, 1].set_ylabel('Words')\n",
    "        \n",
    "        # Word length distributions\n",
    "        for i, split in enumerate(splits):\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in self.dataset[split]]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in self.dataset[split] if isinstance(item, str)]\n",
    "            lengths = [len(text.split()) for text in texts[:1000]]  # Sample for plotting\n",
    "            axes[1, 0].hist(lengths, alpha=0.7, label=split, bins=30, color=colors[i])\n",
    "        \n",
    "        axes[1, 0].set_title('Word Length Distribution (Sample)')\n",
    "        axes[1, 0].set_xlabel('Words per Sample')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Character length distributions\n",
    "        for i, split in enumerate(splits):\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in self.dataset[split]]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in self.dataset[split] if isinstance(item, str)]\n",
    "            lengths = [len(text) for text in texts[:1000]]  # Sample for plotting\n",
    "            axes[1, 1].hist(lengths, alpha=0.7, label=split, bins=30, color=colors[i])\n",
    "        \n",
    "        axes[1, 1].set_title('Character Length Distribution (Sample)')\n",
    "        axes[1, 1].set_xlabel('Characters per Sample')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('data_analysis/dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_vocabulary(self, num_top_words=20):\n",
    "        \"\"\"Analyze vocabulary and word frequencies\"\"\"\n",
    "        if self.dataset is None:\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nAnalyzing vocabulary...\")\n",
    "        \n",
    "        # Combine all texts\n",
    "        all_texts = []\n",
    "        for split_data in self.dataset.values():\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in split_data]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in split_data if isinstance(item, str)]\n",
    "            all_texts.extend(texts)\n",
    "        \n",
    "        # Tokenize and count words\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in tqdm(all_texts[:5000], desc=\"Processing texts\"):  # Sample for speed\n",
    "            # Simple tokenization (split on whitespace and remove punctuation)\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get top words\n",
    "        top_words = word_counts.most_common(num_top_words)\n",
    "        \n",
    "        print(f\"\\nTop {num_top_words} most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"  {word}: {count:,}\")\n",
    "        \n",
    "        # Vocabulary size\n",
    "        vocab_size = len(word_counts)\n",
    "        print(f\"\\nTotal unique words (vocabulary size): {vocab_size:,}\")\n",
    "        \n",
    "        return {\n",
    "            'top_words': top_words,\n",
    "            'vocab_size': vocab_size,\n",
    "            'word_counts': word_counts\n",
    "        }\n",
    "    \n",
    "    def create_wordcloud(self, word_counts, max_words=100):\n",
    "        \"\"\"Create word cloud visualization\"\"\"\n",
    "        # Filter out common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
    "        \n",
    "        filtered_words = {word: count for word, count in word_counts.items() \n",
    "                         if word not in stop_words and len(word) > 2}\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=max_words,\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(filtered_words)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Training Data', fontsize=16)\n",
    "        plt.savefig('data_analysis/wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def export_analysis_report(self, stats, vocab_info):\n",
    "        \"\"\"Export comprehensive analysis report\"\"\"\n",
    "        report = {\n",
    "            'dataset_statistics': stats,\n",
    "            'vocabulary_analysis': {\n",
    "                'vocab_size': vocab_info['vocab_size'],\n",
    "                'top_words': vocab_info['top_words'][:10]\n",
    "            },\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if stats:\n",
    "            total_samples = sum(s['num_samples'] for s in stats.values())\n",
    "            if total_samples < 10000:\n",
    "                report['recommendations'].append(\"Consider increasing dataset size for better model performance\")\n",
    "            \n",
    "            avg_words = np.mean([s['avg_words'] for s in stats.values()])\n",
    "            if avg_words < 50:\n",
    "                report['recommendations'].append(\"Texts are quite short - consider longer passages for better context\")\n",
    "        \n",
    "        if vocab_info and vocab_info['vocab_size'] < 10000:\n",
    "            report['recommendations'].append(\"Limited vocabulary - model may struggle with diverse topics\")\n",
    "        \n",
    "        # Save report\n",
    "        with open('data_analysis/data_analysis_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        print(\"\\nAnalysis report saved to data_analysis/data_analysis_report.json\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Run complete dataset analysis\"\"\"\n",
    "    os.makedirs(\"data_analysis\", exist_ok=True)\n",
    "    analyzer = DataAnalyzer()\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = analyzer.basic_statistics()\n",
    "    \n",
    "    # Plot distributions\n",
    "    analyzer.plot_distributions(stats)\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    vocab_info = analyzer.analyze_vocabulary()\n",
    "    \n",
    "    # Create word cloud\n",
    "    if vocab_info:\n",
    "        analyzer.create_wordcloud(vocab_info['word_counts'])\n",
    "    \n",
    "    # Export report\n",
    "    report = analyzer.export_analysis_report(stats, vocab_info)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated plots and report in the data_analysis folder.\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
