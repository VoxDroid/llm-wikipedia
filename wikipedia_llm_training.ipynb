{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b788a66",
   "metadata": {},
   "source": [
    "# Wikipedia LLM Training Notebook\n",
    "\n",
    "This notebook contains the complete pipeline for training a Wikipedia-based language model using Phi-2 with LoRA fine-tuning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Package Installation](#Package-Installation)\n",
    "2. [GPU Check](#GPU-Check)\n",
    "3. [Data Download](#Data-Download)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Model Setup](#Model-Setup)\n",
    "6. [LoRA Configuration](#LoRA-Configuration)\n",
    "7. [Training Progress Check](#Training-Progress-Check)\n",
    "8. [Model Training](#Model-Training)\n",
    "9. [VRAM Cleanup](#VRAM-Cleanup)\n",
    "10. [Model Testing](#Model-Testing)\n",
    "11. [Chat Interface](#Chat-Interface)\n",
    "12. [Training Restart](#Training-Restart)\n",
    "13. [Hyperparameter Tuning](#Hyperparameter-Tuning)\n",
    "14. [Model Evaluation](#Model-Evaluation)\n",
    "15. [Model Deployment](#Model-Deployment)\n",
    "16. [Data Analysis](#Data-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263af3e",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "\n",
    "Installs all required Python packages for the training pipeline, including PyTorch, Transformers, PEFT, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "!pip install transformers datasets peft accelerate bitsandbytes trl tqdm protobuf scipy sentencepiece psutil matplotlib optuna mlflow rouge-score nltk wordcloud seaborn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600969",
   "metadata": {},
   "source": [
    "## GPU Check\n",
    "\n",
    "Verifies CUDA availability and GPU information to ensure the system is ready for GPU-accelerated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44de41",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "Downloads a subset of the Wikipedia dataset (100,000 articles) from Hugging Face and saves it locally for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_wikipedia_working.py\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Downloading Wikipedia dataset (this will take a while)...\")\n",
    "dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.en\",  # November 2023 snapshot\n",
    "    split=\"train\",\n",
    "    streaming=False  # Set to True if to stream instead of download all\n",
    ")\n",
    "\n",
    "# Take subset for testing (adjust as needed)\n",
    "print(\"Creating subset...\")\n",
    "subset = dataset.select(range(min(100000, len(dataset))))\n",
    "\n",
    "print(\"Saving dataset locally...\")\n",
    "subset.save_to_disk(\"data/wikipedia_100k\")\n",
    "print(f\"Dataset saved! Total articles: {len(subset)}\")\n",
    "print(f\"Sample article title: {subset[0]['title']}\")\n",
    "print(f\"Sample text: {subset[0]['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f002d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Processes the raw Wikipedia data with text cleaning, quality filtering, and multiple instruction formats for better training data quality and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9516b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data.py\n",
    "from datasets import load_from_disk\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load saved dataset\n",
    "dataset = load_from_disk(\"data/wikipedia_100k\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove wiki markup\n",
    "    text = re.sub(r'\\[\\[([^\\]|]*\\|)?([^\\]]*)\\]\\]', r'\\2', text)  # Remove [[links]]\n",
    "    text = re.sub(r\"''+\", '', text)  # Remove italic/bold markup\n",
    "    return text\n",
    "\n",
    "def format_for_training(example):\n",
    "    \"\"\"Convert Wikipedia articles to multiple instruction formats\"\"\"\n",
    "    text = clean_text(example['text'])\n",
    "    title = example['title']\n",
    "    \n",
    "    # Skip very short articles\n",
    "    if len(text) < 200:\n",
    "        return {\"text\": \"\"}  # Will be filtered out\n",
    "    \n",
    "    # Create multiple instruction formats for variety\n",
    "    formats = [\n",
    "        f\"### Instruction:\\nProvide information about {title}.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nExplain what {title} is.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nTell me about {title}.\\n\\n### Response:\\n{text[:1000]}\",\n",
    "        f\"### Instruction:\\nGive me details on {title}.\\n\\n### Response:\\n{text[:1000]}\"\n",
    "    ]\n",
    "    \n",
    "    # Randomly select one format\n",
    "    return {\"text\": random.choice(formats)}\n",
    "\n",
    "# Format dataset\n",
    "print(\"Formatting and cleaning dataset...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    format_for_training,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Filter out empty entries\n",
    "formatted_dataset = formatted_dataset.filter(lambda x: len(x['text']) > 50)\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Save formatted data\n",
    "split_dataset.save_to_disk(\"data/formatted_wikipedia\")\n",
    "print(\"Data preparation complete!\")\n",
    "print(f\"Training samples: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(split_dataset['test'])}\")\n",
    "print(f\"Filtered out {len(dataset) - len(formatted_dataset)} short/invalid articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708807d",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Loads the Phi-2 model with 4-bit quantization, performs system resource checks, memory estimation, and provides model merging capabilities for optimized inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_setup.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "# Choose a base model (recommendations for your hardware)\n",
    "MODEL_OPTIONS = {\n",
    "    \"small\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B - easiest to train\n",
    "    \"medium\": \"microsoft/phi-2\",  # 2.7B - good balance\n",
    "    \"large\": \"mistralai/Mistral-7B-v0.1\"  # 7B - needs quantization\n",
    "}\n",
    "\n",
    "model_name = MODEL_OPTIONS[\"medium\"]  # Start with Phi-2\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "def estimate_memory_usage(model_name, quantized=True):\n",
    "    \"\"\"Estimate memory usage for the model\"\"\"\n",
    "    # Rough estimates (in GB)\n",
    "    base_sizes = {\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\": 2.2,\n",
    "        \"microsoft/phi-2\": 5.0,\n",
    "        \"mistralai/Mistral-7B-v0.1\": 14.0\n",
    "    }\n",
    "    \n",
    "    base_size = base_sizes.get(model_name, 5.0)\n",
    "    if quantized:\n",
    "        estimated_vram = base_size * 0.3  # 4-bit quantization reduces to ~30%\n",
    "    else:\n",
    "        estimated_vram = base_size\n",
    "    \n",
    "    return estimated_vram\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"Check available system resources\"\"\"\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "    cpu_memory = psutil.virtual_memory().total / 1024**3\n",
    "    \n",
    "    print(f\"Available GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"Available CPU Memory: {cpu_memory:.1f} GB\")\n",
    "    return gpu_memory, cpu_memory\n",
    "\n",
    "# Check resources before loading\n",
    "print(\"Checking system resources...\")\n",
    "gpu_mem, cpu_mem = check_system_resources()\n",
    "\n",
    "estimated_vram = estimate_memory_usage(model_name, quantized=True)\n",
    "print(f\"Estimated VRAM usage: {estimated_vram:.1f} GB\")\n",
    "\n",
    "if gpu_mem > 0 and estimated_vram > gpu_mem * 0.8:\n",
    "    print(\"WARNING: Estimated VRAM usage is high. Consider using smaller model or more aggressive quantization.\")\n",
    "elif gpu_mem == 0:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow on CPU.\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05a4d1",
   "metadata": {},
   "source": [
    "## Optional: Model Merging\n",
    "\n",
    "This section provides functionality to merge LoRA weights back into the base model for faster inference after training.\n",
    "\n",
    "### Usage Instructions:\n",
    "\n",
    "1. Train your LoRA model using the training cells below.\n",
    "2. After training completes, run the code cell below to merge the LoRA weights.\n",
    "3. Provide the base model (loaded above), the path to your trained LoRA weights, and a save path for the merged model.\n",
    "4. Example: `merged_model = merge_lora_weights(model, './wikipedia_model/final', './wikipedia_model/merged')`\n",
    "5. The merged model will be saved and can be used for faster inference without LoRA adapters.\n",
    "\n",
    "**Note:** This step is optional. If you prefer to keep the model in LoRA format for flexibility, you can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model merging capability\n",
    "def merge_lora_weights(base_model, lora_path, save_path):\n",
    "    \"\"\"Merge LoRA weights back into base model for faster inference\"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(\"Loading LoRA weights...\")\n",
    "    lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    print(\"Merging weights...\")\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    \n",
    "    print(f\"Saving merged model to {save_path}...\")\n",
    "    merged_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(\"Model merging complete!\")\n",
    "    return merged_model\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# merged_model = merge_lora_weights(model, \"./wikipedia_model/final\", \"./wikipedia_model/merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20c963",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Configures Low-Rank Adaptation with dynamic rank scaling, expanded target modules, and enhanced regularization for optimal fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config.py\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Dynamic LoRA rank based on model size\n",
    "def get_dynamic_lora_rank(model):\n",
    "    \"\"\"Determine LoRA rank based on model parameter count\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if param_count < 2e9:  # < 2B params\n",
    "        rank = 8\n",
    "    elif param_count < 7e9:  # 2-7B params\n",
    "        rank = 16\n",
    "    else:  # > 7B params\n",
    "        rank = 32\n",
    "    \n",
    "    return rank\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Get dynamic rank\n",
    "lora_rank = get_dynamic_lora_rank(model)\n",
    "print(f\"Using LoRA rank: {lora_rank} (based on model size)\")\n",
    "\n",
    "# Enhanced LoRA configuration with more target modules\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,  # Dynamic rank\n",
    "    lora_alpha=lora_rank * 2,  # Typically 2x rank\n",
    "    target_modules=[\n",
    "        \"q_proj\",   # Query projection\n",
    "        \"k_proj\",   # Key projection  \n",
    "        \"v_proj\",   # Value projection\n",
    "        \"o_proj\",   # Output projection\n",
    "        \"gate_proj\",  # Gate projection (for some models)\n",
    "        \"up_proj\",    # Up projection (for some models)\n",
    "        \"down_proj\",  # Down projection (for some models)\n",
    "    ],\n",
    "    lora_dropout=0.1,  # Slightly higher dropout for better regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# LoRA dropout scheduling (optional - requires custom training loop)\n",
    "# This would need to be implemented in the training loop to decrease dropout over time\n",
    "def get_scheduled_dropout(epoch, max_epochs, initial_dropout=0.1, final_dropout=0.01):\n",
    "    \"\"\"Linearly decrease dropout from initial to final over training\"\"\"\n",
    "    return initial_dropout - (initial_dropout - final_dropout) * (epoch / max_epochs)\n",
    "\n",
    "print(f\"\\nLoRA Configuration Summary:\")\n",
    "print(f\"- Rank: {lora_rank}\")\n",
    "print(f\"- Alpha: {lora_rank * 2}\")\n",
    "print(f\"- Target Modules: {len(lora_config.target_modules)} modules\")\n",
    "print(f\"- Dropout: {lora_config.lora_dropout}\")\n",
    "print(\"- Bias: none\")\n",
    "print(\"- Task: Causal LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d4456",
   "metadata": {},
   "source": [
    "## Training Progress Check\n",
    "\n",
    "Analyzes existing checkpoints with loss curve visualization, training speed metrics, and live model performance preview through sample generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e68ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_progress.py\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"Check current training progress with enhanced metrics and visualization\"\"\"\n",
    "    \n",
    "    output_dir = \"./wikipedia_model\"\n",
    "    \n",
    "    # Find all checkpoints\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found. Training hasn't started yet.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    \n",
    "    print(f\"Total checkpoints found: {len(checkpoints)}\")\n",
    "    print(\"\\nCheckpoint history:\")\n",
    "    \n",
    "    # Collect data for plotting\n",
    "    steps = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    eval_losses = []\n",
    "    \n",
    "    for cp in checkpoints:\n",
    "        step = cp.split(\"-\")[-1]\n",
    "        \n",
    "        # Try to read trainer_state.json for more info\n",
    "        state_file = os.path.join(cp, \"trainer_state.json\")\n",
    "        if os.path.exists(state_file):\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "                epoch = state.get('epoch', 'N/A')\n",
    "                print(f\"  - Step {step} (Epoch {epoch:.2f})\")\n",
    "                \n",
    "                # Collect training metrics\n",
    "                log_history = state.get('log_history', [])\n",
    "                training_entries = [entry for entry in log_history if 'loss' in entry]\n",
    "                eval_entries = [entry for entry in log_history if 'eval_loss' in entry]\n",
    "                \n",
    "                if training_entries:\n",
    "                    last_train = training_entries[-1]\n",
    "                    steps.append(int(step))\n",
    "                    losses.append(last_train.get('loss', 0))\n",
    "                    learning_rates.append(last_train.get('learning_rate', 0))\n",
    "                \n",
    "                if eval_entries:\n",
    "                    last_eval = eval_entries[-1]\n",
    "                    eval_losses.append(last_eval.get('eval_loss', 0))\n",
    "        else:\n",
    "            print(f\"  - Step {step}\")\n",
    "    \n",
    "    # Latest checkpoint\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"\\nLatest checkpoint: {latest}\")\n",
    "    \n",
    "    # Read detailed info from latest\n",
    "    state_file = os.path.join(latest, \"trainer_state.json\")\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            print(f\"\\nDetailed Progress:\")\n",
    "            print(f\"  Current Step: {state.get('global_step', 'N/A')}\")\n",
    "            print(f\"  Current Epoch: {state.get('epoch', 'N/A'):.2f}\")\n",
    "            \n",
    "            # Get latest training metrics (find last entry with training loss)\n",
    "            log_history = state.get('log_history', [])\n",
    "            training_entries = [entry for entry in log_history if 'loss' in entry]\n",
    "            eval_entries = [entry for entry in log_history if 'eval_loss' in entry]\n",
    "            \n",
    "            if training_entries:\n",
    "                last_training = training_entries[-1]\n",
    "                training_loss = last_training.get('loss', 'N/A')\n",
    "                learning_rate = last_training.get('learning_rate', 'N/A')\n",
    "            else:\n",
    "                training_loss = 'N/A'\n",
    "                learning_rate = 'N/A'\n",
    "            \n",
    "            if eval_entries:\n",
    "                last_eval = eval_entries[-1]\n",
    "                validation_loss = last_eval.get('eval_loss', 'N/A')\n",
    "            else:\n",
    "                validation_loss = 'N/A'\n",
    "            \n",
    "            print(f\"  Training Loss: {training_loss}\")\n",
    "            print(f\"  Validation Loss: {validation_loss}\")\n",
    "            print(f\"  Learning Rate: {learning_rate}\")\n",
    "            \n",
    "            # Calculate progress percentage\n",
    "            max_steps = state.get('max_steps', None)\n",
    "            if max_steps:\n",
    "                current_step = state.get('global_step', 0)\n",
    "                percentage = (current_step / max_steps) * 100\n",
    "                print(f\"  Progress: {percentage:.1f}% ({current_step}/{max_steps} steps)\")\n",
    "                \n",
    "                # Estimate training speed\n",
    "                if len(steps) > 1:\n",
    "                    steps_per_hour = (steps[-1] - steps[0]) / ((len(steps) - 1) * 0.1)  # Rough estimate\n",
    "                    print(f\"  Estimated Speed: ~{steps_per_hour:.1f} steps/hour\")\n",
    "            else:\n",
    "                print(\"  Progress: Unable to calculate (max_steps not available)\")\n",
    "            \n",
    "            # Plot loss curves if we have data\n",
    "            if len(losses) > 1:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                \n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.plot(steps, losses, 'b-', label='Training Loss')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Training Loss Curve')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.plot(steps, learning_rates, 'r-', label='Learning Rate')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('Learning Rate Schedule')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                if eval_losses:\n",
    "                    eval_steps = steps[:len(eval_losses)]  # Assume eval at same steps\n",
    "                    plt.plot(eval_steps, eval_losses, 'g-', label='Validation Loss')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Validation Loss')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Show recent training history (last 3 training entries with validation loss)\n",
    "            if training_entries:\n",
    "                print(f\"\\nRecent Training History (last {min(3, len(training_entries))} entries):\")\n",
    "                for entry in training_entries[-3:]:\n",
    "                    step = entry.get('step', 'N/A')\n",
    "                    loss = entry.get('loss', 'N/A')\n",
    "                    lr = entry.get('learning_rate', 'N/A')\n",
    "                    epoch = entry.get('epoch', 'N/A')\n",
    "                    \n",
    "                    # Find corresponding validation loss for this step\n",
    "                    val_loss = 'N/A'\n",
    "                    for eval_entry in eval_entries:\n",
    "                        if eval_entry.get('step') == step:\n",
    "                            val_loss = eval_entry.get('eval_loss', 'N/A')\n",
    "                            break\n",
    "                    \n",
    "                    print(f\"    Step {step}: Train Loss={loss}, Val Loss={val_loss}, LR={lr}, Epoch={epoch:.2f}\")\n",
    "            else:\n",
    "                print(\"\\nNo training history available yet.\")\n",
    "            \n",
    "            # Generate sample output from latest checkpoint\n",
    "            print(f\"\\nSample Generation from Latest Checkpoint:\")\n",
    "            try:\n",
    "                # Load model for inference\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"microsoft/phi-2\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                lora_model = PeftModel.from_pretrained(base_model, latest)\n",
    "                test_tokenizer = AutoTokenizer.from_pretrained(latest)\n",
    "                \n",
    "                # Test prompt\n",
    "                prompt = \"### Instruction:\\nProvide information about Artificial Intelligence.\\n\\n### Response:\\n\"\n",
    "                inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = lora_model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=100,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=test_tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # Extract just the response part\n",
    "                answer = response.split(\"### Response:\\n\")[-1][:200] + \"...\"\n",
    "                print(f\"AI: {answer}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del base_model, lora_model\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate sample: {e}\")\n",
    "    \n",
    "    print(\"\\nTo resume training, simply run: python train.py\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f73d9f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Executes the main training loop with optimized settings including gradient checkpointing, mixed precision, and automatic checkpoint resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c487cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_vram():\n",
    "    \"\"\"Clear VRAM before training\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"VRAM cleared\")\n",
    "\n",
    "# Call it before loading model\n",
    "clear_vram()\n",
    "\n",
    "# Load formatted data\n",
    "dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust based on your needs\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wikipedia_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,     # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=16,     # Effective batch size = 128\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                         # Mixed precision training\n",
    "    save_steps=100,                    # Save every 100 steps\n",
    "    eval_steps=100,                    # Evaluate every 100 steps\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,                # Keep only last 3 checkpoints (saves disk space)\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",          # Memory efficient optimizer\n",
    "    resume_from_checkpoint=True,       # Auto-resume from latest checkpoint\n",
    "    gradient_checkpointing=True,       # Enable gradient checkpointing for memory efficiency\n",
    "    dataloader_num_workers=2,          # Use 2 workers for data loading\n",
    "    logging_dir=\"./logs\",              # Directory for TensorBoard logs\n",
    "    report_to=[\"tensorboard\"],         # Enable TensorBoard logging\n",
    "    seed=42,                           # Set seed for reproducibility\n",
    "    weight_decay=0.01,                 # Add weight decay for regularization\n",
    "    max_grad_norm=1.0,                 # Gradient clipping\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory\"\"\"\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    latest = checkpoints[-1]\n",
    "    return latest\n",
    "\n",
    "# Look for existing checkpoint\n",
    "checkpoint_path = find_latest_checkpoint(\"./wikipedia_model\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "    print(f\"Progress: Step {checkpoint_path.split('-')[-1]}\")\n",
    "    \n",
    "    # Resume training from checkpoint\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found. Starting fresh training...\")\n",
    "    # Start training from scratch\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# Set model to eval mode\n",
    "# model.eval() ## training sets it to eval automatically\n",
    "\n",
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./wikipedia_model/final\")\n",
    "tokenizer.save_pretrained(\"./wikipedia_model/final\")\n",
    "# Also save the LoRA weights explicitly\n",
    "model.save_pretrained(\"./wikipedia_model/final\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total training time: {training_duration:.2f} seconds ({training_duration/3600:.2f} hours)\")\n",
    "print(f\"Model saved to: ./wikipedia_model/final\")\n",
    "print(f\"TensorBoard logs: ./logs (run 'tensorboard --logdir ./logs' to view)\")\n",
    "print(f\"Checkpoints saved every {training_args.save_steps} steps\")\n",
    "print(f\"Final step: {trainer.state.global_step}\")\n",
    "print(f\"Final epoch: {trainer.state.epoch:.2f}\")\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e73f1",
   "metadata": {},
   "source": [
    "## VRAM Cleanup\n",
    "\n",
    "Clears GPU memory cache and forces garbage collection to free up VRAM after training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart_training.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def restart_training():\n",
    "    \"\"\"Delete all training files to restart from scratch\"\"\"\n",
    "    \n",
    "    # Directories to delete\n",
    "    dirs_to_delete = [\"./wikipedia_model\", \"./logs\"]\n",
    "    \n",
    "    for dir_path in dirs_to_delete:\n",
    "        if os.path.exists(dir_path):\n",
    "            print(f\"Deleting {dir_path}...\")\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Deleted {dir_path}\")\n",
    "        else:\n",
    "            print(f\"{dir_path} does not exist\")\n",
    "    \n",
    "    print(\"\\nAll training files deleted. You can now run the training cell to start fresh.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"This will delete all training checkpoints and logs. Are you sure? (yes/no): \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        restart_training()\n",
    "    else:\n",
    "        print(\"Restart cancelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5002ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_vram.py\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_vram():\n",
    "    \"\"\"Clear VRAM completely\"\"\"\n",
    "    print(\"Clearing VRAM...\")\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"VRAM allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"VRAM reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(\"VRAM cleared!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99203de5",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Loads a trained checkpoint and tests the model's ability to generate informative responses about various topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text, max_length=512):\n",
    "    \"\"\"Calculate perplexity for a given text\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_rouge(generated, reference):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores\"\"\"\n",
    "    # BLEU\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothing)\n",
    "    \n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, generated)\n",
    "    \n",
    "    return bleu, rouge_scores\n",
    "\n",
    "def load_checkpoint_model(checkpoint_path):\n",
    "    \"\"\"Load model from checkpoint\"\"\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_prompts):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    results = {\n",
    "        'perplexity': [],\n",
    "        'bleu': [],\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': [],\n",
    "        'responses': []\n",
    "    }\n",
    "    \n",
    "    for prompt_data in test_prompts:\n",
    "        prompt = prompt_data['prompt']\n",
    "        reference = prompt_data['reference']\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=len(inputs['input_ids'][0]) + 100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answer = response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        perplexity = calculate_perplexity(model, tokenizer, generated_answer)\n",
    "        bleu, rouge_scores = calculate_bleu_rouge(generated_answer, reference)\n",
    "        \n",
    "        results['perplexity'].append(perplexity)\n",
    "        results['bleu'].append(bleu)\n",
    "        results['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "        results['responses'].append({\n",
    "            'prompt': prompt,\n",
    "            'generated': generated_answer,\n",
    "            'reference': reference\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    for key in ['perplexity', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "        results[f'avg_{key}'] = np.mean(results[key])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts with references for evaluation\n",
    "test_prompts = [\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nProvide information about Python programming language.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Python is a high-level programming language known for its simplicity and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nExplain what Artificial Intelligence is.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. It includes learning, reasoning, and self-correction.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nTell me about Machine Learning.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Machine Learning is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\\n\",\n",
    "        'reference': \"The capital of France is Paris, which is located in the north-central part of the country along the Seine River.\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"### Instruction:\\nDescribe the process of photosynthesis.\\n\\n### Response:\\n\",\n",
    "        'reference': \"Photosynthesis is the process by which plants convert light energy into chemical energy. It involves chlorophyll absorbing sunlight and using it to convert carbon dioxide and water into glucose and oxygen.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Find all checkpoints for comparison\n",
    "checkpoints = glob.glob(\"./wikipedia_model/checkpoint-*\")\n",
    "checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"Available checkpoints:\")\n",
    "    for i, cp in enumerate(checkpoints):\n",
    "        print(f\"{i+1}. {cp}\")\n",
    "    \n",
    "    # Test latest checkpoint\n",
    "    latest_checkpoint = checkpoints[-1]\n",
    "    print(f\"\\nTesting latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    model, tokenizer = load_checkpoint_model(latest_checkpoint)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_model(model, tokenizer, test_prompts)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average Perplexity: {results['avg_perplexity']:.2f}\")\n",
    "    print(f\"Average BLEU Score: {results['avg_bleu']:.4f}\")\n",
    "    print(f\"Average ROUGE-1 F1: {results['avg_rouge1']:.4f}\")\n",
    "    print(f\"Average ROUGE-2 F1: {results['avg_rouge2']:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1: {results['avg_rougeL']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show sample responses\n",
    "    print(\"\\nSample Responses:\")\n",
    "    for i, resp in enumerate(results['responses'][:3]):\n",
    "        print(f\"\\n{i+1}. Prompt: {resp['prompt'].split('### Instruction:\\n')[1].split('\\n\\n### Response:\\n')[0]}\")\n",
    "        print(f\"   Generated: {resp['generated'][:100]}...\")\n",
    "        print(f\"   Reference: {resp['reference'][:100]}...\")\n",
    "    \n",
    "    # Checkpoint comparison (if multiple checkpoints exist)\n",
    "    if len(checkpoints) > 1:\n",
    "        print(f\"\\nCheckpoint Comparison (last {min(3, len(checkpoints))} checkpoints):\")\n",
    "        comparison_results = []\n",
    "        \n",
    "        for cp in checkpoints[-3:]:\n",
    "            try:\n",
    "                model_comp, tokenizer_comp = load_checkpoint_model(cp)\n",
    "                results_comp = evaluate_model(model_comp, tokenizer_comp, test_prompts[:2])  # Quick comparison\n",
    "                step = cp.split(\"-\")[-1]\n",
    "                comparison_results.append({\n",
    "                    'step': step,\n",
    "                    'perplexity': results_comp['avg_perplexity'],\n",
    "                    'bleu': results_comp['avg_bleu']\n",
    "                })\n",
    "                del model_comp\n",
    "                torch.cuda.empty_cache()\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for res in comparison_results:\n",
    "            print(f\"Step {res['step']}: Perplexity={res['perplexity']:.2f}, BLEU={res['bleu']:.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoints found. Please train the model first.\")\n",
    "\n",
    "print(\"\\nModel testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668de446",
   "metadata": {},
   "source": [
    "## Chat Interface\n",
    "\n",
    "Creates an interactive chat interface for real-time conversation with the trained Wikipedia model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface.py\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class SafeChatInterface:\n",
    "    def __init__(self, model_path=\"./wikipedia_model/checkpoint-100\"):\n",
    "        # Create text generation pipeline\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_path,\n",
    "            tokenizer=model_path,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        \n",
    "        # Conversation history\n",
    "        self.history = []\n",
    "        self.max_history = 10\n",
    "        \n",
    "        # Safety patterns\n",
    "        self.safety_patterns = [\n",
    "            r'\\b(hate|violence|kill|murder|suicide)\\b',\n",
    "            r'\\b(drug|weapon|nuclear)\\b.*\\b(make|build|create)\\b',\n",
    "            r'\\b(hack|exploit|crack)\\b.*\\b(password|system|account)\\b'\n",
    "        ]\n",
    "    \n",
    "    def check_safety(self, text):\n",
    "        \"\"\"Check if response contains unsafe content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        for pattern in self.safety_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                return False, \"Response contains potentially unsafe content.\"\n",
    "        return True, \"\"\n",
    "    \n",
    "    def filter_response(self, response):\n",
    "        \"\"\"Filter and clean the response\"\"\"\n",
    "        # Extract just the response part\n",
    "        if \"### Response:\\n\" in response:\n",
    "            answer = response.split(\"### Response:\\n\")[-1]\n",
    "        else:\n",
    "            answer = response\n",
    "        \n",
    "        # Remove any instruction-like prefixes that might leak through\n",
    "        answer = re.sub(r'### Instruction:.*?\\n', '', answer)\n",
    "        \n",
    "        # Limit length\n",
    "        if len(answer) > 1000:\n",
    "            answer = answer[:1000] + \"...\"\n",
    "        \n",
    "        return answer.strip()\n",
    "    \n",
    "    def generate_streaming(self, prompt, max_length=300):\n",
    "        \"\"\"Generate response with simulated streaming\"\"\"\n",
    "        inputs = self.pipe.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        \n",
    "        generated_tokens = inputs['input_ids'][0].tolist()\n",
    "        \n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "        \n",
    "        for i in range(max_length - len(generated_tokens)):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.pipe.model(**inputs)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # Apply temperature and top-p sampling\n",
    "                next_token_logits = next_token_logits / 0.7\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Get top tokens\n",
    "                top_probs, top_tokens = torch.topk(probs, 50, dim=-1)\n",
    "                top_probs = top_probs / top_probs.sum()\n",
    "                \n",
    "                # Sample from top tokens\n",
    "                next_token = torch.multinomial(top_probs, 1).item()\n",
    "                next_token = top_tokens[0, next_token].item()\n",
    "                \n",
    "                if next_token == self.pipe.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                token_text = self.pipe.tokenizer.decode([next_token], skip_special_tokens=True)\n",
    "                print(token_text, end=\"\", flush=True)\n",
    "                \n",
    "                # Update inputs for next iteration\n",
    "                inputs['input_ids'] = torch.tensor([generated_tokens]).to(inputs['input_ids'].device)\n",
    "        \n",
    "        print()  # New line\n",
    "        return self.pipe.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    def chat(self):\n",
    "        print(\"Wikipedia LLM - Enhanced Chat Interface\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Features: Conversation history, safety checks, streaming responses\")\n",
    "        print(\"Commands: 'history' to view chat history, 'clear' to reset, 'quit' to exit\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            elif query.lower() == 'history':\n",
    "                print(\"\\nConversation History:\")\n",
    "                for i, (q, a) in enumerate(self.history[-5:], 1):  # Show last 5\n",
    "                    print(f\"{i}. Q: {q[:50]}...\")\n",
    "                    print(f\"   A: {a[:50]}...\")\n",
    "                continue\n",
    "            elif query.lower() == 'clear':\n",
    "                self.history.clear()\n",
    "                print(\"Conversation history cleared.\")\n",
    "                continue\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # Create prompt with context\n",
    "            context = \"\"\n",
    "            if self.history:\n",
    "                # Add last 2 exchanges for context\n",
    "                recent_history = self.history[-2:]\n",
    "                context = \"Previous conversation:\\n\"\n",
    "                for q, a in recent_history:\n",
    "                    context += f\"User: {q}\\nAssistant: {a}\\n\"\n",
    "                context += \"\\n\"\n",
    "            \n",
    "            prompt = f\"{context}### Instruction:\\nProvide information about {query}.\\n\\n### Response:\\n\"\n",
    "            \n",
    "            try:\n",
    "                # Generate response with streaming\n",
    "                full_response = self.generate_streaming(prompt, max_length=300)\n",
    "                \n",
    "                # Filter and check safety\n",
    "                answer = self.filter_response(full_response)\n",
    "                is_safe, safety_msg = self.check_safety(answer)\n",
    "                \n",
    "                if not is_safe:\n",
    "                    print(f\"Warning: {safety_msg}\")\n",
    "                    answer = \"I apologize, but I cannot provide information on that topic.\"\n",
    "                \n",
    "                # Add to history\n",
    "                self.history.append((query, answer))\n",
    "                if len(self.history) > self.max_history:\n",
    "                    self.history.pop(0)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating response: {e}\")\n",
    "                answer = \"I encountered an error. Please try again.\"\n",
    "            \n",
    "            # The response is already printed via streaming\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_interface = SafeChatInterface()\n",
    "    chat_interface.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b989c",
   "metadata": {},
   "source": [
    "## Training Restart\n",
    "\n",
    "Safely deletes all training checkpoints and logs to allow restarting the training process from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart_training.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def restart_training():\n",
    "    \"\"\"Delete all training files to restart from scratch\"\"\"\n",
    "    \n",
    "    # Directories to delete\n",
    "    dirs_to_delete = [\"./wikipedia_model\", \"./logs\"]\n",
    "    \n",
    "    for dir_path in dirs_to_delete:\n",
    "        if os.path.exists(dir_path):\n",
    "            print(f\"Deleting {dir_path}...\")\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Deleted {dir_path}\")\n",
    "        else:\n",
    "            print(f\"{dir_path} does not exist\")\n",
    "    \n",
    "    print(\"\\nAll training files deleted. You can now run the training cell to start fresh.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"This will delete all training checkpoints and logs. Are you sure? (yes/no): \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        restart_training()\n",
    "    else:\n",
    "        print(\"Restart cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b794f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Automated hyperparameter optimization using grid search to find the best learning rate, batch size, and LoRA parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter_tuning.py\n",
    "import itertools\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_OPTIONS = {\n",
    "    \"small\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B - easiest to train\n",
    "    \"medium\": \"microsoft/phi-2\",  # 2.7B - good balance\n",
    "    \"large\": \"mistralai/Mistral-7B-v0.1\"  # 7B - needs quantization\n",
    "}\n",
    "\n",
    "model_name = MODEL_OPTIONS[\"medium\"]  # Start with Phi-2\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for hyperparameter evaluation\"\"\"\n",
    "    \n",
    "    learning_rate, per_device_batch_size, lora_rank, lora_alpha, weight_decay = params\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # For causal LM, labels are the same as input_ids\n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    tokenized_train = dataset[\"train\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    tokenized_test = dataset[\"test\"].select(range(200)).map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    # Calculate effective batch size\n",
    "    gradient_accumulation_steps = max(1, 16 // per_device_batch_size)  # Target effective batch of 16\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./grid_trials\",\n",
    "        num_train_epochs=1,  # Quick trial\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=True,\n",
    "        save_steps=50,\n",
    "        eval_steps=50,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        seed=42,\n",
    "        report_to=[],  # Disable integrations to avoid MLflow issues\n",
    "        remove_unused_columns=False,  # Keep all columns\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA with parameters\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Load model (assuming model and tokenizer are already loaded in notebook)\n",
    "    model_with_lora = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_with_lora,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Get best eval loss\n",
    "    best_eval_loss = trainer.state.best_metric\n",
    "    \n",
    "    # Cleanup\n",
    "    del model_with_lora\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_eval_loss, params\n",
    "\n",
    "def run_hyperparameter_tuning():\n",
    "    \"\"\"Run grid search hyperparameter optimization\"\"\"\n",
    "    print(\"Starting hyperparameter tuning with grid search...\")\n",
    "    print(\"This may take several hours depending on the number of combinations.\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "        'per_device_batch_size': [4, 8],\n",
    "        'lora_rank': [8, 16],\n",
    "        'lora_alpha': [16, 32],\n",
    "        'weight_decay': [0.01, 0.05]\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    combinations = list(itertools.product(*values))\n",
    "    \n",
    "    print(f\"Testing {len(combinations)} parameter combinations...\")\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    for i, combo in enumerate(tqdm(combinations, desc=\"Hyperparameter Tuning\", unit=\"trial\")):\n",
    "        params = dict(zip(keys, combo))\n",
    "        print(f\"\\nTrial {i+1}/{len(combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            loss, _ = objective(combo)\n",
    "            print(f\"Eval loss: {loss:.4f}\")\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_params = params\n",
    "                print(f\"New best loss: {best_loss:.4f} (Trial {i+1})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best eval loss: {best_loss:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    with open(\"best_hyperparams.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nBest parameters saved to best_hyperparams.json\")\n",
    "    print(\"Use these parameters in your main training script.\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Uncomment to run tuning (takes significant time and resources)\n",
    "if __name__ == \"__main__\":\n",
    "    best_params = run_hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0332f",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Comprehensive model evaluation with multiple metrics including BLEU, ROUGE, perplexity, and custom benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_evaluation.py\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation suite\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.smooth = SmoothingFunction().method4\n",
    "    \n",
    "    def calculate_perplexity(self, texts, batch_size=8):\n",
    "        \"\"\"Calculate perplexity on a set of texts\"\"\"\n",
    "        perplexities = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = encodings.input_ids.to(self.device)\n",
    "            attention_mask = encodings.attention_mask.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "                loss = outputs.loss\n",
    "                perplexity = torch.exp(loss).item()\n",
    "                perplexities.append(perplexity)\n",
    "        \n",
    "        return np.mean(perplexities)\n",
    "    \n",
    "    def calculate_bleu_rouge(self, generated_texts, reference_texts):\n",
    "        \"\"\"Calculate BLEU and ROUGE scores\"\"\"\n",
    "        bleu_scores = []\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for gen, ref in zip(generated_texts, reference_texts):\n",
    "            # BLEU score\n",
    "            gen_tokens = gen.split()\n",
    "            ref_tokens = ref.split()\n",
    "            bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=self.smooth)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # ROUGE scores\n",
    "            rouge_scores = self.rouge_scorer.score(ref, gen)\n",
    "            rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "            rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "            rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "        return {\n",
    "            'bleu': np.mean(bleu_scores),\n",
    "            'rouge1': np.mean(rouge1_scores),\n",
    "            'rouge2': np.mean(rouge2_scores),\n",
    "            'rougeL': np.mean(rougeL_scores)\n",
    "        }\n",
    "    \n",
    "    def generate_samples(self, prompts, num_samples=10):\n",
    "        \"\"\"Generate text samples for qualitative evaluation\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        for prompt in tqdm(prompts[:num_samples], desc=\"Generating samples\"):\n",
    "            try:\n",
    "                outputs = self.generator(\n",
    "                    prompt,\n",
    "                    max_new_tokens=100,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_text = outputs[0]['generated_text']\n",
    "                samples.append({\n",
    "                    'prompt': prompt,\n",
    "                    'generated': generated_text,\n",
    "                    'length': len(generated_text.split())\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating for prompt: {e}\")\n",
    "                samples.append({\n",
    "                    'prompt': prompt,\n",
    "                    'generated': \"Error generating text\",\n",
    "                    'length': 0\n",
    "                })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def benchmark_model(self, test_dataset=None, num_samples=100):\n",
    "        \"\"\"Run comprehensive benchmark\"\"\"\n",
    "        print(\"Starting comprehensive model evaluation...\")\n",
    "        \n",
    "        if test_dataset is None:\n",
    "            # Load test dataset\n",
    "            dataset = load_from_disk(\"data/formatted_wikipedia\")\n",
    "            test_dataset = dataset[\"test\"]\n",
    "        \n",
    "        # Sample test data\n",
    "        test_texts = test_dataset.select(range(min(num_samples, len(test_dataset))))\n",
    "        test_prompts = [item['text'][:200] + \"...\" for item in test_texts]  # Use first 200 chars as prompts\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        print(\"Calculating perplexity...\")\n",
    "        perplexity = self.calculate_perplexity([item['text'] for item in test_texts])\n",
    "        \n",
    "        # Generate samples for BLEU/ROUGE\n",
    "        print(\"Generating samples for BLEU/ROUGE evaluation...\")\n",
    "        generated_samples = self.generate_samples(test_prompts, num_samples=50)\n",
    "        \n",
    "        # Extract generated and reference texts\n",
    "        generated_texts = [sample['generated'] for sample in generated_samples]\n",
    "        reference_texts = [item['text'] for item in test_texts[:50]]\n",
    "        \n",
    "        # Calculate BLEU and ROUGE\n",
    "        print(\"Calculating BLEU and ROUGE scores...\")\n",
    "        text_metrics = self.calculate_bleu_rouge(generated_texts, reference_texts)\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'perplexity': perplexity,\n",
    "            'bleu_score': text_metrics['bleu'],\n",
    "            'rouge1_score': text_metrics['rouge1'],\n",
    "            'rouge2_score': text_metrics['rouge2'],\n",
    "            'rougeL_score': text_metrics['rougeL'],\n",
    "            'samples': generated_samples[:5],  # Save first 5 samples\n",
    "            'num_samples_evaluated': len(generated_samples)\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Perplexity: {results['perplexity']:.4f}\")\n",
    "        print(f\"BLEU Score: {results['bleu_score']:.4f}\")\n",
    "        print(f\"ROUGE-1 Score: {results['rouge1_score']:.4f}\")\n",
    "        print(f\"ROUGE-2 Score: {results['rouge2_score']:.4f}\")\n",
    "        print(f\"ROUGE-L Score: {results['rougeL_score']:.4f}\")\n",
    "        print(f\"Samples evaluated: {results['num_samples_evaluated']}\")\n",
    "        \n",
    "        # Save results\n",
    "        with open(\"evaluation_results.json\", \"w\") as f:\n",
    "            # Convert numpy types to native Python types for JSON serialization\n",
    "            json_results = {\n",
    "                k: float(v) if isinstance(v, np.floating) else v \n",
    "                for k, v in results.items() \n",
    "                if k != 'samples'\n",
    "            }\n",
    "            json_results['samples'] = results['samples']\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(\"Results saved to evaluation_results.json\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_evaluation_history(self, checkpoint_dirs=None):\n",
    "        \"\"\"Plot evaluation metrics across checkpoints\"\"\"\n",
    "        if checkpoint_dirs is None:\n",
    "            checkpoint_dirs = [d for d in os.listdir(\"./wikipedia_model\") if d.startswith(\"checkpoint-\")]\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        \n",
    "        perplexities = []\n",
    "        steps = []\n",
    "        \n",
    "        for checkpoint in checkpoint_dirs:\n",
    "            checkpoint_path = f\"./wikipedia_model/{checkpoint}\"\n",
    "            if os.path.exists(f\"{checkpoint_path}/evaluation_results.json\"):\n",
    "                with open(f\"{checkpoint_path}/evaluation_results.json\", \"r\") as f:\n",
    "                    results = json.load(f)\n",
    "                    perplexities.append(results.get('perplexity', 0))\n",
    "                    steps.append(int(checkpoint.split(\"-\")[1]))\n",
    "        \n",
    "        if perplexities:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(steps, perplexities, marker='o')\n",
    "            plt.xlabel('Training Steps')\n",
    "            plt.ylabel('Perplexity')\n",
    "            plt.title('Model Perplexity Across Training Checkpoints')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('evaluation_history.png')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No evaluation results found in checkpoints\")\n",
    "\n",
    "# Usage example\n",
    "def run_model_evaluation():\n",
    "    \"\"\"Run comprehensive model evaluation\"\"\"\n",
    "    evaluator = ModelEvaluator(model, tokenizer)\n",
    "    results = evaluator.benchmark_model()\n",
    "    \n",
    "    # Plot evaluation history if multiple checkpoints exist\n",
    "    evaluator.plot_evaluation_history()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run evaluation\n",
    "# if __name__ == \"__main__\":\n",
    "#     results = run_model_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca49ae",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Export and deploy your trained model for production use with ONNX, quantization, and containerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_deployment.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelDeployer:\n",
    "    \"\"\"Model deployment utilities for production\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, output_dir=\"./deployed_model\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def merge_and_save_model(self):\n",
    "        \"\"\"Merge LoRA weights with base model and save\"\"\"\n",
    "        print(\"Merging LoRA weights with base model...\")\n",
    "        \n",
    "        # Merge LoRA weights\n",
    "        merged_model = self.model.merge_and_unload()\n",
    "        \n",
    "        # Save merged model\n",
    "        merged_dir = self.output_dir / \"merged_model\"\n",
    "        merged_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        merged_model.save_pretrained(merged_dir)\n",
    "        self.tokenizer.save_pretrained(merged_dir)\n",
    "        \n",
    "        print(f\"Merged model saved to {merged_dir}\")\n",
    "        return merged_dir\n",
    "    \n",
    "    def export_to_onnx(self, merged_model_path=None, opset_version=14):\n",
    "        \"\"\"Export model to ONNX format\"\"\"\n",
    "        print(\"Exporting model to ONNX format...\")\n",
    "        \n",
    "        if merged_model_path is None:\n",
    "            merged_model_path = self.merge_and_save_model()\n",
    "        \n",
    "        from transformers.onnx import export\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        \n",
    "        # Load merged model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            merged_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Export to ONNX\n",
    "        onnx_path = self.output_dir / \"model.onnx\"\n",
    "        \n",
    "        # Create dummy input\n",
    "        dummy_input = self.tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "        \n",
    "        # Export\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "            onnx_path,\n",
    "            opset_version=opset_version,\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"ONNX model exported to {onnx_path}\")\n",
    "        return onnx_path\n",
    "    \n",
    "    def quantize_onnx_model(self, onnx_path=None):\n",
    "        \"\"\"Quantize ONNX model for better performance\"\"\"\n",
    "        print(\"Quantizing ONNX model...\")\n",
    "        \n",
    "        if onnx_path is None:\n",
    "            onnx_path = self.output_dir / \"model.onnx\"\n",
    "        \n",
    "        quantized_path = self.output_dir / \"model_quantized.onnx\"\n",
    "        \n",
    "        quantize_dynamic(\n",
    "            model_input=str(onnx_path),\n",
    "            model_output=str(quantized_path),\n",
    "            weight_type=QuantType.QInt8\n",
    "        )\n",
    "        \n",
    "        print(f\"Quantized model saved to {quantized_path}\")\n",
    "        return quantized_path\n",
    "    \n",
    "    def create_dockerfile(self):\n",
    "        \"\"\"Create Dockerfile for containerized deployment\"\"\"\n",
    "        dockerfile_content = '''FROM python:3.9-slim\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model files\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "'''\n",
    "        \n",
    "        dockerfile_path = self.output_dir / \"Dockerfile\"\n",
    "        with open(dockerfile_path, \"w\") as f:\n",
    "            f.write(dockerfile_content)\n",
    "        \n",
    "        print(f\"Dockerfile created at {dockerfile_path}\")\n",
    "        return dockerfile_path\n",
    "    \n",
    "    def create_requirements_txt(self):\n",
    "        \"\"\"Create requirements.txt for deployment\"\"\"\n",
    "        requirements = '''transformers>=4.21.0\n",
    "torch>=1.12.0\n",
    "accelerate>=0.20.0\n",
    "fastapi>=0.95.0\n",
    "uvicorn>=0.21.0\n",
    "pydantic>=1.10.0\n",
    "onnxruntime>=1.14.0\n",
    "numpy>=1.21.0\n",
    "'''\n",
    "        \n",
    "        req_path = self.output_dir / \"requirements.txt\"\n",
    "        with open(req_path, \"w\") as f:\n",
    "            f.write(requirements)\n",
    "        \n",
    "        print(f\"requirements.txt created at {req_path}\")\n",
    "        return req_path\n",
    "    \n",
    "    def create_fastapi_app(self):\n",
    "        \"\"\"Create FastAPI application for serving the model\"\"\"\n",
    "        app_content = '''from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "app = FastAPI(title=\"Wikipedia LLM API\", version=\"1.0.0\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"./merged_model\")\n",
    "\n",
    "try:\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=MODEL_PATH,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    generator = None\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_length: int = 100\n",
    "    temperature: float = 0.7\n",
    "    num_return_sequences: int = 1\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    generated_texts: list[str]\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    if generator is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "    \n",
    "    try:\n",
    "        outputs = generator(\n",
    "            request.prompt,\n",
    "            max_new_tokens=request.max_length,\n",
    "            temperature=request.temperature,\n",
    "            num_return_sequences=request.num_return_sequences,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_texts = [output[\"generated_text\"] for output in outputs]\n",
    "        return GenerationResponse(generated_texts=generated_texts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"model_loaded\": generator is not None}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "        \n",
    "        app_path = self.output_dir / \"app.py\"\n",
    "        with open(app_path, \"w\") as f:\n",
    "            f.write(app_content)\n",
    "        \n",
    "        print(f\"FastAPI app created at {app_path}\")\n",
    "        return app_path\n",
    "    \n",
    "    def create_deployment_package(self):\n",
    "        \"\"\"Create complete deployment package\"\"\"\n",
    "        print(\"Creating deployment package...\")\n",
    "        \n",
    "        # Merge and save model\n",
    "        merged_path = self.merge_and_save_model()\n",
    "        \n",
    "        # Create requirements.txt\n",
    "        self.create_requirements_txt()\n",
    "        \n",
    "        # Create FastAPI app\n",
    "        self.create_fastapi_app()\n",
    "        \n",
    "        # Create Dockerfile\n",
    "        self.create_dockerfile()\n",
    "        \n",
    "        # Create README\n",
    "        readme_content = '''# Wikipedia LLM Deployment\n",
    "\n",
    "This directory contains the deployment package for the fine-tuned Wikipedia LLM.\n",
    "\n",
    "## Local Deployment\n",
    "\n",
    "1. Install dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. Run the API server:\n",
    "   ```bash\n",
    "   python app.py\n",
    "   ```\n",
    "\n",
    "3. Test the API:\n",
    "   ```bash\n",
    "   curl -X POST \"http://localhost:8000/generate\" \\\\\n",
    "        -H \"Content-Type: application/json\" \\\\\n",
    "        -d '{\"prompt\": \"The history of artificial intelligence\", \"max_length\": 50}'\n",
    "   ```\n",
    "\n",
    "## Docker Deployment\n",
    "\n",
    "1. Build the Docker image:\n",
    "   ```bash\n",
    "   docker build -t wikipedia-llm .\n",
    "   ```\n",
    "\n",
    "2. Run the container:\n",
    "   ```bash\n",
    "   docker run -p 8000:8000 wikipedia-llm\n",
    "   ```\n",
    "\n",
    "## API Endpoints\n",
    "\n",
    "- `POST /generate`: Generate text from a prompt\n",
    "- `GET /health`: Health check endpoint\n",
    "\n",
    "## Model Information\n",
    "\n",
    "- Base model: Microsoft Phi-2\n",
    "- Fine-tuned on: Wikipedia dataset\n",
    "- Framework: Transformers\n",
    "- Quantization: 4-bit (during training)\n",
    "'''\n",
    "        \n",
    "        readme_path = self.output_dir / \"README.md\"\n",
    "        with open(readme_path, \"w\") as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"Deployment package created in {self.output_dir}\")\n",
    "        print(\"Files created:\")\n",
    "        print(\"  - merged_model/ (merged model weights)\")\n",
    "        print(\"  - requirements.txt\")\n",
    "        print(\"  - app.py (FastAPI application)\")\n",
    "        print(\"  - Dockerfile\")\n",
    "        print(\"  - README.md\")\n",
    "        \n",
    "        return self.output_dir\n",
    "\n",
    "def deploy_model():\n",
    "    \"\"\"Complete model deployment workflow\"\"\"\n",
    "    deployer = ModelDeployer(model, tokenizer)\n",
    "    \n",
    "    # Create deployment package\n",
    "    deployment_dir = deployer.create_deployment_package()\n",
    "    \n",
    "    print(f\"\\nDeployment package ready at: {deployment_dir}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Test locally: python app.py\")\n",
    "    print(\"2. Build Docker: docker build -t wikipedia-llm .\")\n",
    "    print(\"3. Deploy to cloud platform of choice\")\n",
    "    \n",
    "    return deployment_dir\n",
    "\n",
    "# Uncomment to create deployment package\n",
    "# if __name__ == \"__main__\":\n",
    "#     deploy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bf12b",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Analyze your training dataset statistics, distribution, and quality metrics to understand your model's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_analysis.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import os\n",
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"Comprehensive dataset analysis tools\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path=\"data/formatted_wikipedia\"):\n",
    "        try:\n",
    "            self.dataset = load_from_disk(dataset_path)\n",
    "            print(f\"Loaded dataset with {len(self.dataset)} splits\")\n",
    "            \n",
    "            # Debug: Check dataset structure\n",
    "            if len(self.dataset) > 0:\n",
    "                first_split = list(self.dataset.keys())[0]\n",
    "                first_item = self.dataset[first_split][0]\n",
    "                print(f\"First item type: {type(first_item)}\")\n",
    "                if isinstance(first_item, dict):\n",
    "                    print(f\"First item keys: {list(first_item.keys())}\")\n",
    "                else:\n",
    "                    print(f\"First item (first 100 chars): {str(first_item)[:100]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            self.dataset = None\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Calculate basic dataset statistics\"\"\"\n",
    "        if self.dataset is None:\n",
    "            return None\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        for split_name, split_data in self.dataset.items():\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in split_data]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in split_data if isinstance(item, str)]\n",
    "            \n",
    "            # Text lengths\n",
    "            text_lengths = [len(text.split()) for text in texts]\n",
    "            \n",
    "            stats[split_name] = {\n",
    "                'num_samples': len(texts),\n",
    "                'avg_words': np.mean(text_lengths),\n",
    "                'min_words': np.min(text_lengths),\n",
    "                'max_words': np.max(text_lengths),\n",
    "                'median_words': np.median(text_lengths),\n",
    "                'total_words': sum(text_lengths),\n",
    "                'avg_chars': np.mean([len(text) for text in texts])\n",
    "            }\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for split, stat in stats.items():\n",
    "            print(f\"\\n{split.upper()} SPLIT:\")\n",
    "            print(f\"  Samples: {stat['num_samples']:,}\")\n",
    "            print(f\"  Avg words per sample: {stat['avg_words']:.1f}\")\n",
    "            print(f\"  Word count range: {stat['min_words']}-{stat['max_words']}\")\n",
    "            print(f\"  Median words: {stat['median_words']:.1f}\")\n",
    "            print(f\"  Total words: {stat['total_words']:,}\")\n",
    "            print(f\"  Avg characters per sample: {stat['avg_chars']:.1f}\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_distributions(self, stats):\n",
    "        \"\"\"Plot data distributions\"\"\"\n",
    "        if stats is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Dataset Analysis', fontsize=16)\n",
    "        \n",
    "        splits = list(stats.keys())\n",
    "        colors = ['blue', 'green', 'red', 'orange']\n",
    "        \n",
    "        # Sample counts\n",
    "        axes[0, 0].bar(splits, [stats[s]['num_samples'] for s in splits], color=colors[:len(splits)])\n",
    "        axes[0, 0].set_title('Number of Samples per Split')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Average words\n",
    "        axes[0, 1].bar(splits, [stats[s]['avg_words'] for s in splits], color=colors[:len(splits)])\n",
    "        axes[0, 1].set_title('Average Words per Sample')\n",
    "        axes[0, 1].set_ylabel('Words')\n",
    "        \n",
    "        # Word length distributions\n",
    "        for i, split in enumerate(splits):\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in self.dataset[split]]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in self.dataset[split] if isinstance(item, str)]\n",
    "            lengths = [len(text.split()) for text in texts[:1000]]  # Sample for plotting\n",
    "            axes[1, 0].hist(lengths, alpha=0.7, label=split, bins=30, color=colors[i])\n",
    "        \n",
    "        axes[1, 0].set_title('Word Length Distribution (Sample)')\n",
    "        axes[1, 0].set_xlabel('Words per Sample')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Character length distributions\n",
    "        for i, split in enumerate(splits):\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in self.dataset[split]]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in self.dataset[split] if isinstance(item, str)]\n",
    "            lengths = [len(text) for text in texts[:1000]]  # Sample for plotting\n",
    "            axes[1, 1].hist(lengths, alpha=0.7, label=split, bins=30, color=colors[i])\n",
    "        \n",
    "        axes[1, 1].set_title('Character Length Distribution (Sample)')\n",
    "        axes[1, 1].set_xlabel('Characters per Sample')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('data_analysis/dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_vocabulary(self, num_top_words=20):\n",
    "        \"\"\"Analyze vocabulary and word frequencies\"\"\"\n",
    "        if self.dataset is None:\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nAnalyzing vocabulary...\")\n",
    "        \n",
    "        # Combine all texts\n",
    "        all_texts = []\n",
    "        for split_data in self.dataset.values():\n",
    "            # Handle both dict format {'text': '...'} and string format\n",
    "            try:\n",
    "                # Try dict format first\n",
    "                texts = [item['text'] for item in split_data]\n",
    "            except (TypeError, KeyError):\n",
    "                # If that fails, assume items are strings directly\n",
    "                texts = [item for item in split_data if isinstance(item, str)]\n",
    "            all_texts.extend(texts)\n",
    "        \n",
    "        # Tokenize and count words\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in tqdm(all_texts[:5000], desc=\"Processing texts\"):  # Sample for speed\n",
    "            # Simple tokenization (split on whitespace and remove punctuation)\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get top words\n",
    "        top_words = word_counts.most_common(num_top_words)\n",
    "        \n",
    "        print(f\"\\nTop {num_top_words} most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"  {word}: {count:,}\")\n",
    "        \n",
    "        # Vocabulary size\n",
    "        vocab_size = len(word_counts)\n",
    "        print(f\"\\nTotal unique words (vocabulary size): {vocab_size:,}\")\n",
    "        \n",
    "        return {\n",
    "            'top_words': top_words,\n",
    "            'vocab_size': vocab_size,\n",
    "            'word_counts': word_counts\n",
    "        }\n",
    "    \n",
    "    def create_wordcloud(self, word_counts, max_words=100):\n",
    "        \"\"\"Create word cloud visualization\"\"\"\n",
    "        # Filter out common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
    "        \n",
    "        filtered_words = {word: count for word, count in word_counts.items() \n",
    "                         if word not in stop_words and len(word) > 2}\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=max_words,\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(filtered_words)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Training Data', fontsize=16)\n",
    "        plt.savefig('data_analysis/wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def export_analysis_report(self, stats, vocab_info):\n",
    "        \"\"\"Export comprehensive analysis report\"\"\"\n",
    "        report = {\n",
    "            'dataset_statistics': stats,\n",
    "            'vocabulary_analysis': {\n",
    "                'vocab_size': vocab_info['vocab_size'],\n",
    "                'top_words': vocab_info['top_words'][:10]\n",
    "            },\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if stats:\n",
    "            total_samples = sum(s['num_samples'] for s in stats.values())\n",
    "            if total_samples < 10000:\n",
    "                report['recommendations'].append(\"Consider increasing dataset size for better model performance\")\n",
    "            \n",
    "            avg_words = np.mean([s['avg_words'] for s in stats.values()])\n",
    "            if avg_words < 50:\n",
    "                report['recommendations'].append(\"Texts are quite short - consider longer passages for better context\")\n",
    "        \n",
    "        if vocab_info and vocab_info['vocab_size'] < 10000:\n",
    "            report['recommendations'].append(\"Limited vocabulary - model may struggle with diverse topics\")\n",
    "        \n",
    "        # Save report\n",
    "        with open('data_analysis/data_analysis_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        print(\"\\nAnalysis report saved to data_analysis/data_analysis_report.json\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Run complete dataset analysis\"\"\"\n",
    "    os.makedirs(\"data_analysis\", exist_ok=True)\n",
    "    analyzer = DataAnalyzer()\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = analyzer.basic_statistics()\n",
    "    \n",
    "    # Plot distributions\n",
    "    analyzer.plot_distributions(stats)\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    vocab_info = analyzer.analyze_vocabulary()\n",
    "    \n",
    "    # Create word cloud\n",
    "    if vocab_info:\n",
    "        analyzer.create_wordcloud(vocab_info['word_counts'])\n",
    "    \n",
    "    # Export report\n",
    "    report = analyzer.export_analysis_report(stats, vocab_info)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated plots and report in the data_analysis folder.\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
